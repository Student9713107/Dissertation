{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor, BaseDRRegressor\n",
    "from causalml.inference.tree import CausalRandomForestRegressor\n",
    "from causalml.metrics import get_cumgain, auuc_score, plot_gain\n",
    "from causalml.dataset import synthetic_data\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.utils.extmath import cartesian\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "palette = ['plum', 'g', 'orange', 'r', 'b', 'yellow', 'cyan', 'white']\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm, uniform\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import softmax as scipy_softmax\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from causalml.inference.meta.base import BaseLearner\n",
    "from causalml.inference.meta.utils import (\n",
    "    check_treatment_vector,\n",
    "    check_p_conditions,\n",
    "    convert_pd_to_np,\n",
    ")\n",
    "from causalml.metrics import regression_metrics\n",
    "from causalml.propensity import compute_propensity_score\n",
    "\n",
    "logger = logging.getLogger(\"causalml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRTLearner(BaseLearner):\n",
    "    \"\"\"A parent class for H-learner regressor classes.\n",
    "\n",
    "    A H-learner estimates treatment effects with three machine learning models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        control_effect_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a H-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (outcome_learner is not None)\n",
    "            and (control_effect_learner is not None)\n",
    "            and (treatment_effect_learner is not None)\n",
    "        )\n",
    "\n",
    "        if outcome_learner is None:\n",
    "            self.model_mu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu = outcome_learner\n",
    "\n",
    "        if control_effect_learner is None:\n",
    "            self.model_tau_c = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau_c = control_effect_learner\n",
    "\n",
    "        if treatment_effect_learner is None:\n",
    "            self.model_tau_t = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau_t = treatment_effect_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(outcome_learner={},\\n\"\n",
    "            \"\\tcontrol_effect_learner={},\\n\"\n",
    "            \"\\ttreatment_effect_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu.__repr__(),\n",
    "                self.model_tau_c.__repr__(),\n",
    "                self.model_tau_t.__repr__(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        if p is None:\n",
    "            self._set_propensity_models(X=X, treatment=treatment, y=y)\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "        self.models_tau_c = {\n",
    "            group: deepcopy(self.model_tau_c) for group in self.t_groups\n",
    "        }\n",
    "        self.models_tau_t = {\n",
    "            group: deepcopy(self.model_tau_t) for group in self.t_groups\n",
    "        }\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_mu.fit(X, y)\n",
    "\n",
    "        for group in self.t_groups:\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            X_filt = X[mask]\n",
    "            y_filt = y[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "\n",
    "            # Calculate variances and treatment effects\n",
    "            var_c = (\n",
    "                y_filt[w == 0] - self.model_mu.predict(X_filt[w == 0])\n",
    "            ).var()\n",
    "            self.vars_c[group] = var_c\n",
    "            var_t = (\n",
    "                y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1])\n",
    "            ).var()\n",
    "            self.vars_t[group] = var_t\n",
    "\n",
    "            # Train treatment models\n",
    "            d_c = (self.model_mu.predict(X_filt[w == 0]) - y_filt[w == 0])\n",
    "            d_t = (y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1]))\n",
    "            self.models_tau_c[group].fit(X_filt[w == 0], d_c)\n",
    "            self.models_tau_t[group].fit(X_filt[w == 1], d_t)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            logger.info(\"Generating propensity score\")\n",
    "            p = dict()\n",
    "            for group in self.t_groups:\n",
    "                p_model = self.propensity_model[group]\n",
    "                p[group] = p_model.predict(X)\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "        dhat_cs = {}\n",
    "        dhat_ts = {}\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            model_tau_c = self.models_tau_c[group]\n",
    "            model_tau_t = self.models_tau_t[group]\n",
    "            dhat_cs[group] = model_tau_c.predict(X)\n",
    "            dhat_ts[group] = model_tau_t.predict(X)\n",
    "\n",
    "            _te = (dhat_cs[group] + dhat_ts[group]).reshape(\n",
    "                -1, 1\n",
    "            )\n",
    "            te[:, i] = np.ravel(_te)\n",
    "\n",
    "            if (y is not None) and (treatment is not None) and verbose:\n",
    "                mask = (treatment == group) | (treatment == self.control_name)\n",
    "                treatment_filt = treatment[mask]\n",
    "                X_filt = X[mask]\n",
    "                y_filt = y[mask]\n",
    "                w = (treatment_filt == group).astype(int)\n",
    "\n",
    "                yhat = np.zeros_like(y, dtype=float)\n",
    "                yhat = self.model_mu.predict(X)\n",
    "\n",
    "                logger.info(\"Error metrics for group {}\".format(group))\n",
    "                regression_metrics(y, yhat, w)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, dhat_cs, dhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, p=p, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            models_tau_c_global = deepcopy(self.models_tau_c)\n",
    "            models_tau_t_global = deepcopy(self.models_tau_t)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_tau_c = deepcopy(models_tau_c_global)\n",
    "            self.models_tau_t = deepcopy(models_tau_t_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=self.propensity, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=p, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, dhat_cs, dhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            _ate = te[:, i].mean()\n",
    "\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "            prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "            dhat_c = dhat_cs[group][mask]\n",
    "            dhat_t = dhat_ts[group][mask]\n",
    "            p_filt = p[group][mask]\n",
    "\n",
    "            # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "            # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "            se = np.sqrt(\n",
    "                (\n",
    "                    self.vars_t[group] / prob_treatment\n",
    "                    + self.vars_c[group] / (1 - prob_treatment)\n",
    "                    + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "                )\n",
    "                / w.shape[0]\n",
    "            )\n",
    "\n",
    "            _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "            _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "            ate[i] = _ate\n",
    "            ate_lb[i] = _ate_lb\n",
    "            ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            models_tau_c_global = deepcopy(self.models_tau_c)\n",
    "            models_tau_t_global = deepcopy(self.models_tau_t)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_tau_c = deepcopy(models_tau_c_global)\n",
    "            self.models_tau_t = deepcopy(models_tau_t_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "class BaseRTRegressor(BaseRTLearner):\n",
    "    \"\"\"\n",
    "    A parent class for H-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        control_effect_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize an X-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            outcome_learner=outcome_learner,\n",
    "            control_effect_learner=control_effect_learner,\n",
    "            treatment_effect_learner=treatment_effect_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePLearner(BaseLearner):\n",
    "    \"\"\"A parent class for P-learner regressor classes.\n",
    "\n",
    "    A P-learner estimates treatment effects with one machine learning model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a P-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None)\n",
    "\n",
    "        self.model_nu = deepcopy(learner)\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(pairwise_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_nu.__repr__(),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def pair_data(self, X, treatment, y, num_samples):\n",
    "\n",
    "        # Split T=0 and T=1\n",
    "        X0 = X[treatment == 0]\n",
    "        Y0 = y[treatment == 0]\n",
    "        X1 = X[treatment == 1]\n",
    "        Y1 = y[treatment == 1]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X1_scaled = scaler.fit_transform(X1)\n",
    "        X0_scaled = scaler.fit_transform(X0)\n",
    "\n",
    "        # Build KD-tree from Y\n",
    "        tree = cKDTree(X1_scaled)\n",
    "\n",
    "        # Query for k nearest neighbours\n",
    "        distances, indices = tree.query(X0_scaled, k=num_samples)\n",
    "        # Randomly sample rows from X_treated for each row in X_control\n",
    "        random_indices = np.random.randint(0, len(X1), (len(X0), num_samples))\n",
    "        \n",
    "        # Expand T=0 data to match shape\n",
    "        X0_expanded = np.repeat(X0, num_samples, axis=0)  # Repeat X0\n",
    "        Y0_expanded = np.repeat(Y0, num_samples, axis=0)  # Repeat Y0\n",
    "\n",
    "        # Get corresponding T=1 data (using the random indices)\n",
    "        X1_sampled = X1[indices.flatten()]\n",
    "        Y1_sampled = Y1[indices.flatten()]\n",
    "\n",
    "        # Reshape the sampled X1 and Y1 for the final DataFrame\n",
    "        X1_sampled = X1_sampled.reshape(-1, X.shape[1])\n",
    "        Y1_sampled = Y1_sampled.reshape(-1, 1)\n",
    "\n",
    "        # Combine the matrices (X0 with X1, Y0 with Y1)\n",
    "        X_combined = np.concatenate([X0_expanded, X1_sampled], axis=1)  # Concatenate feature matrices\n",
    "        Y_combined = np.column_stack((Y0_expanded, Y1_sampled))  # Concatenate outcome matrices\n",
    "\n",
    "        # Calculate outcome differences\n",
    "        nu = Y_combined[:,1] - Y_combined[:,0]\n",
    "\n",
    "        return X_combined, nu\n",
    "\n",
    "    def fit(self, X, treatment, y, p, num_samples=3):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (optional): number of treated samples to pair with each control.\n",
    "        \"\"\"\n",
    "        \n",
    "        X, y = convert_pd_to_np(X, y)\n",
    "        X_pair, nu = self.pair_data(X, treatment, y, num_samples)\n",
    "\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_nu.fit(X_pair, nu)\n",
    "\n",
    "        # for group in self.t_groups:\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     X_filt = X[mask]\n",
    "        #     y_filt = y[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "\n",
    "        #     # Calculate variances and treatment effects\n",
    "        #     var_c = (\n",
    "        #         y_filt[w == 0] - self.model_mu.predict(X_filt[w == 0])\n",
    "        #     ).var()\n",
    "        #     self.vars_c[group] = var_c\n",
    "        #     var_t = (\n",
    "        #         y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1])\n",
    "        #     ).var()\n",
    "        #     self.vars_t[group] = var_t\n",
    "\n",
    "        #     # Train treatment models\n",
    "        #     d_c = (self.model_mu.predict(X_filt[w == 0]) - y_filt[w == 0])\n",
    "        #     d_t = (y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1]))\n",
    "        #     self.models_tau_c[group].fit(X_filt[w == 0], d_c)\n",
    "        #     self.models_tau_t[group].fit(X_filt[w == 1], d_t)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        X = np.concatenate([X, X], axis=1)\n",
    "\n",
    "        # if p is None:\n",
    "        #     logger.info(\"Generating propensity score\")\n",
    "        #     p = dict()\n",
    "        #     for group in self.t_groups:\n",
    "        #         p_model = self.propensity_model[group]\n",
    "        #         p[group] = p_model.predict(X)\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "        dhat_cs = {}\n",
    "        dhat_ts = {}\n",
    "\n",
    "        _te = (self.model_nu.predict(X)).reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        te[:, 0] = np.ravel(_te)\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     model_tau_c = self.models_tau_c[group]\n",
    "        #     model_tau_t = self.models_tau_t[group]\n",
    "        #     dhat_cs[group] = model_tau_c.predict(X)\n",
    "        #     dhat_ts[group] = model_tau_t.predict(X)\n",
    "\n",
    "        #     _te = (dhat_cs[group] + dhat_ts[group]).reshape(\n",
    "        #         -1, 1\n",
    "        #     )\n",
    "        #     te[:, i] = np.ravel(_te)\n",
    "\n",
    "        #     if (y is not None) and (treatment is not None) and verbose:\n",
    "        #         mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #         treatment_filt = treatment[mask]\n",
    "        #         X_filt = X[mask]\n",
    "        #         y_filt = y[mask]\n",
    "        #         w = (treatment_filt == group).astype(int)\n",
    "\n",
    "        #         yhat = np.zeros_like(y, dtype=float)\n",
    "        #         yhat = self.model_mu.predict(X)\n",
    "\n",
    "        #         logger.info(\"Error metrics for group {}\".format(group))\n",
    "        #         regression_metrics(y, yhat, w)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, dhat_cs, dhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=3,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p, num_samples)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, p=p, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=self.propensity, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=p, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, dhat_cs, dhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        ate[0] = te[:, 0].mean()\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     _ate = te[:, i].mean()\n",
    "\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "        #     prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "        #     dhat_c = dhat_cs[group][mask]\n",
    "        #     dhat_t = dhat_ts[group][mask]\n",
    "        #     p_filt = p[group][mask]\n",
    "\n",
    "        #     # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "        #     # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "        #     se = np.sqrt(\n",
    "        #         (\n",
    "        #             self.vars_t[group] / prob_treatment\n",
    "        #             + self.vars_c[group] / (1 - prob_treatment)\n",
    "        #             + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "        #         )\n",
    "        #         / w.shape[0]\n",
    "        #     )\n",
    "\n",
    "        #     _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "        #     _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "        #     ate[i] = _ate\n",
    "        #     ate_lb[i] = _ate_lb\n",
    "        #     ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "class BasePRegressor(BasePLearner):\n",
    "    \"\"\"\n",
    "    A parent class for P-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a P-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRPLearner(BaseLearner):\n",
    "    \"\"\"A parent class for PH-learner regressor classes.\n",
    "\n",
    "    A PH-learner estimates treatment effects with two machine learning models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        pair_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a H-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            pair_learner (optional): a model to estimate pair outcome differences\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (outcome_learner is not None)\n",
    "            and (pair_learner is not None)\n",
    "        )\n",
    "\n",
    "        if outcome_learner is None:\n",
    "            self.model_mu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu = outcome_learner\n",
    "\n",
    "        if pair_learner is None:\n",
    "            self.model_nu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_nu = pair_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(outcome_learner={},\\n\"\n",
    "            \"\\tpair_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu.__repr__(),\n",
    "                self.model_nu.__repr__()\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def pair_data(self, X, treatment, y, num_samples):\n",
    "\n",
    "        # Split T=0 and T=1\n",
    "        X0 = X[treatment == 0]\n",
    "        Y0 = y[treatment == 0]\n",
    "        X1 = X[treatment == 1]\n",
    "        Y1 = y[treatment == 1]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X1_scaled = scaler.fit_transform(X1)\n",
    "        X0_scaled = scaler.fit_transform(X0)\n",
    "\n",
    "        # Build KD-tree from Y\n",
    "        tree = cKDTree(X1_scaled)\n",
    "\n",
    "        # Query for k nearest neighbours\n",
    "        distances, indices = tree.query(X0_scaled, k=num_samples)\n",
    "        # Randomly sample rows from X_treated for each row in X_control\n",
    "        random_indices = np.random.randint(0, len(X1), (len(X0), num_samples))\n",
    "\n",
    "        # Expand T=0 data to match shape\n",
    "        X0_expanded = np.repeat(X0, num_samples, axis=0)  # Repeat X0 for 5 times\n",
    "        Y0_expanded = np.repeat(Y0, num_samples, axis=0)  # Repeat Y0 for 5 times\n",
    "\n",
    "        # Get corresponding T=1 data (using the random indices)\n",
    "        X1_sampled = X1[indices.flatten()]\n",
    "        Y1_sampled = Y1[indices.flatten()]\n",
    "\n",
    "        # Reshape the sampled X1 and Y1 for the final DataFrame\n",
    "        X1_sampled = X1_sampled.reshape(-1, X.shape[1])\n",
    "        Y1_sampled = Y1_sampled.reshape(-1, 1)\n",
    "\n",
    "        # Combine the matrices (X0 with X1, Y0 with Y1)\n",
    "        X_combined = np.concatenate([X0_expanded, X1_sampled], axis=1)  # Concatenate feature matrices\n",
    "        Y_combined = np.column_stack((Y0_expanded, Y1_sampled))  # Concatenate outcome matrices\n",
    "\n",
    "        # Calculate outcome differences\n",
    "        nu = Y_combined[:,1] - self.model_mu.predict(X1_sampled) + self.model_mu.predict(X0_expanded) - Y_combined[:,0]\n",
    "\n",
    "        return X_combined, nu\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None, num_samples=5):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int, optional): number of pairs for each control observation\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        # if p is None:\n",
    "        #     self._set_propensity_models(X=X, treatment=treatment, y=y)\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_mu.fit(X, y)\n",
    "\n",
    "        # Train pair model\n",
    "        X_paired, nu = self.pair_data(X, treatment, y, num_samples)\n",
    "        self.model_nu.fit(X_paired, nu)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "        X= convert_pd_to_np(X)\n",
    "        X_paired = np.concatenate([X, X], axis=1)\n",
    "\n",
    "        # if p is None:\n",
    "        #     logger.info(\"Generating propensity score\")\n",
    "        #     p = dict()\n",
    "        #     for group in self.t_groups:\n",
    "        #         p_model = self.propensity_model[group]\n",
    "        #         p[group] = p_model.predict(X)\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "\n",
    "        fhat = self.model_mu.predict(X)\n",
    "        tauhat = self.model_nu.predict(X_paired)\n",
    "\n",
    "        _te = (tauhat).reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        te[:, 0] = np.ravel(_te)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, fhat\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=5,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int): number of pairs for each control observation\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p, num_samples)\n",
    "\n",
    "        # if p is None:\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=5,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int): number of pairs for each control observation\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, fhat = self.predict(\n",
    "                    X, treatment, y, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, fhat = self.predict(\n",
    "                    X, treatment, y, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, fhat = self.fit_predict(\n",
    "                X, treatment, y, num_samples, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        # if p is None:\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        ate[0] = te[:, 0].mean()\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     _ate = te[:, i].mean()\n",
    "\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "        #     prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "        #     dhat_c = dhat_cs[group][mask]\n",
    "        #     dhat_t = dhat_ts[group][mask]\n",
    "        #     p_filt = p[group][mask]\n",
    "\n",
    "        #     # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "        #     # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "        #     se = np.sqrt(\n",
    "        #         (\n",
    "        #             self.vars_t[group] / prob_treatment\n",
    "        #             + self.vars_c[group] / (1 - prob_treatment)\n",
    "        #             + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "        #         )\n",
    "        #         / w.shape[0]\n",
    "        #     )\n",
    "\n",
    "        #     _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "        #     _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "        #     ate[i] = _ate\n",
    "        #     ate_lb[i] = _ate_lb\n",
    "        #     ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_nu = deepcopy(model_nu_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "class BaseRPRegressor(BaseRPLearner):\n",
    "    \"\"\"\n",
    "    A parent class for PH-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        pair_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a PH-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            pair_learner (optional): a model to estimate paired treatment effects\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            outcome_learner=outcome_learner,\n",
    "            pair_learner=pair_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePDRLearner(BaseLearner):\n",
    "    \"\"\"\n",
    "    A parent class for PDR-learner regressor classes.\n",
    "    PDR-learner estimates treatment effects with machine learning models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        control_outcome_learner=None,\n",
    "        treatment_outcome_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a PDR-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): model used for all tasks if specific learners are not provided.\n",
    "            control_outcome_learner (optional): model for control outcomes.\n",
    "            treatment_outcome_learner (optional): model for treated outcomes.\n",
    "            treatment_effect_learner (optional): model for treatment effects.\n",
    "            ate_alpha (float, optional): significance level for ATE CI.\n",
    "            control_name (str or int, optional): label for control group.\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (control_outcome_learner is not None)\n",
    "            and (treatment_outcome_learner is not None)\n",
    "            and (treatment_effect_learner is not None)\n",
    "        )\n",
    "\n",
    "        if control_outcome_learner is None:\n",
    "            self.model_mu_c = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu_c = control_outcome_learner\n",
    "\n",
    "        if treatment_outcome_learner is None:\n",
    "            self.model_mu_t = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu_t = treatment_outcome_learner\n",
    "\n",
    "        if treatment_effect_learner is None:\n",
    "            self.model_tau = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau = treatment_effect_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(control_outcome_learner={},\\n\"\n",
    "            \"\\ttreatment_outcome_learner={},\\n\"\n",
    "            \"\\ttreatment_effect_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu_c.__repr__(),\n",
    "                self.model_mu_t.__repr__(),\n",
    "                self.model_tau.__repr__(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None, seed=None):\n",
    "        \"\"\"\n",
    "        Fit the PDR-learner with the doubly robust pairwise estimator.\n",
    "\n",
    "        Args:\n",
    "            X (np.array or pd.DataFrame): feature matrix.\n",
    "            treatment (np.array or pd.Series): treatment vector.\n",
    "            y (np.array or pd.Series): outcome vector.\n",
    "            p (optional): propensity scores; if None, they are estimated.\n",
    "            seed (int): random seed.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        # Use 3-fold cross-fitting\n",
    "        cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "        split_indices = [index for _, index in cv.split(y)]\n",
    "\n",
    "        self.models_mu_c = [deepcopy(self.model_mu_c) for _ in range(3)]\n",
    "        self.models_mu_t = {\n",
    "            group: [deepcopy(self.model_mu_t) for _ in range(3)]\n",
    "            for group in self.t_groups\n",
    "        }\n",
    "        self.models_tau = {\n",
    "            group: [deepcopy(self.model_tau) for _ in range(3)]\n",
    "            for group in self.t_groups\n",
    "        }\n",
    "\n",
    "        # Initialize propensity score container if not provided\n",
    "        if p is None:\n",
    "            self.propensity = {group: np.zeros(y.shape[0]) for group in self.t_groups}\n",
    "\n",
    "        # Cross-fit\n",
    "        for ifold in range(3):\n",
    "            treatment_idx = split_indices[ifold]\n",
    "            outcome_idx = split_indices[(ifold + 1) % 3]\n",
    "            tau_idx = split_indices[(ifold + 2) % 3]\n",
    "\n",
    "            treatment_treat = treatment[treatment_idx]\n",
    "            treatment_out = treatment[outcome_idx]\n",
    "            treatment_tau = treatment[tau_idx]\n",
    "\n",
    "            y_out, y_tau = y[outcome_idx], y[tau_idx]\n",
    "            X_treat, X_out, X_tau = X[treatment_idx], X[outcome_idx], X[tau_idx]\n",
    "\n",
    "            # Propensity score estimation if not provided\n",
    "            if p is None:\n",
    "                logger.info(\"Estimating propensity scores\")\n",
    "                cur_p = dict()\n",
    "                for group in self.t_groups:\n",
    "                    mask = (treatment_treat == group) | (treatment_treat == self.control_name)\n",
    "                    X_filt = X_treat[mask]\n",
    "                    treatment_filt = treatment_treat[mask]\n",
    "                    w_filt = (treatment_filt == group).astype(int)\n",
    "                    # Compute propensity scores for group 'group'\n",
    "                    cur_p[group], _ = compute_propensity_score(\n",
    "                        X=X_filt, treatment=w_filt, X_pred=X_tau, treatment_pred=(treatment_tau == group).astype(int)\n",
    "                    )\n",
    "                    self.propensity[group][tau_idx] = cur_p[group]\n",
    "            else:\n",
    "                cur_p = dict()\n",
    "                if isinstance(p, (np.ndarray, pd.Series)):\n",
    "                    cur_p = {self.t_groups[0]: convert_pd_to_np(p[tau_idx])}\n",
    "                else:\n",
    "                    cur_p = {g: p[g][tau_idx] for g in self.t_groups}\n",
    "                check_p_conditions(cur_p, self.t_groups)\n",
    "\n",
    "            # Outcome regression: fit control and treatment models on outcome fold\n",
    "            logger.info(\"Fitting outcome regressions\")\n",
    "            # Fit control outcome model on control units\n",
    "            self.models_mu_c[ifold].fit(\n",
    "                X_out[treatment_out == self.control_name],\n",
    "                y_out[treatment_out == self.control_name],\n",
    "            )\n",
    "            for group in self.t_groups:\n",
    "                # Fit treated outcome model for each group\n",
    "                self.models_mu_t[group][ifold].fit(\n",
    "                    X_out[treatment_out == group],\n",
    "                    y_out[treatment_out == group],\n",
    "                )\n",
    "\n",
    "            # Fit pseudo outcomes for treatment effect using doubly robust pairwise estimator\n",
    "            logger.info(\"Fitting doubly robust pairwise pseudo outcomes\")\n",
    "            for group in self.t_groups:\n",
    "                # Filter tau-fold: keep observations from control or group 'group'\n",
    "                mask = (treatment_tau == group) | (treatment_tau == self.control_name)\n",
    "                X_filt = X_tau[mask]\n",
    "                y_filt = y_tau[mask]\n",
    "                p_filt = cur_p[group][mask]\n",
    "                # Predict outcomes using outcome models\n",
    "                mu_c_all = self.models_mu_c[ifold].predict(X_filt)\n",
    "                mu_t_all = self.models_mu_t[group][ifold].predict(X_filt)\n",
    "\n",
    "                # Separate into control and treated subsets for pairing\n",
    "                mask_control = (treatment_tau[mask] == self.control_name)\n",
    "                mask_treated = (treatment_tau[mask] == group)\n",
    "                if np.sum(mask_control) == 0 or np.sum(mask_treated) == 0:\n",
    "                    logger.warning(\"Not enough data for pairing in group {}\".format(group))\n",
    "                    continue\n",
    "\n",
    "                X_control = X_filt[mask_control]\n",
    "                y_control = y_filt[mask_control]\n",
    "                p_control = p_filt[mask_control]\n",
    "                mu_control = mu_c_all[mask_control]\n",
    "\n",
    "                X_treated = X_filt[mask_treated]\n",
    "                y_treated = y_filt[mask_treated]\n",
    "                p_treated = p_filt[mask_treated]\n",
    "                mu_treated = mu_t_all[mask_treated]\n",
    "\n",
    "                scaler = MinMaxScaler()\n",
    "\n",
    "                X_control_scaled = scaler.fit_transform(X_control)\n",
    "                X_treated_scaled = scaler.fit_transform(X_treated)\n",
    "\n",
    "                # Build KDTree on the control group\n",
    "                tree = cKDTree(X_control_scaled)\n",
    "\n",
    "                k = 10\n",
    "\n",
    "                # Find the nearest control neighbour for each treated sample\n",
    "                distances, idx_control = tree.query(X_treated_scaled, k=k)  # k=10\n",
    "\n",
    "                # Expand treated indices to match the number of pairs (each treated unit repeats k times)\n",
    "                idx_treated = np.repeat(np.arange(len(X_treated)), k)\n",
    "                idx_control = idx_control.flatten()  # Flatten to align with repeated treated indices\n",
    "\n",
    "\n",
    "                # Compute the doubly robust pairwise pseudo outcome for each pair:\n",
    "                #   dr_pair = [mu_treated - mu_control] +\n",
    "                #             [ (y_treated - mu_treated) / p_treated - (y_control - mu_control) / (1-p_control) ]\n",
    "                dr_pairs = (\n",
    "                    (mu_treated[idx_treated] - mu_control[idx_control])\n",
    "                    + ((y_treated[idx_treated] - mu_treated[idx_treated]) / p_treated[idx_treated]\n",
    "                       - (y_control[idx_control] - mu_control[idx_control]) / (1 - p_control[idx_control]))\n",
    "                )\n",
    "                # Combine the paired features. Here we simply concatenate the control and treated features.\n",
    "                X_pairs = np.hstack([X_control[idx_control], X_treated[idx_treated]])\n",
    "                # Fit the treatment effect learner on the paired data and doubly robust pseudo outcomes.\n",
    "                self.models_tau[group][ifold].fit(X_pairs, dr_pairs)\n",
    "\n",
    "    def predict(self, X, treatment=None, y=None, p=None, return_components=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Predict treatment effects.\n",
    "        Args:\n",
    "            X (np.array or pd.DataFrame): feature matrix.\n",
    "            treatment (optional): treatment vector.\n",
    "            y (optional): outcome vector.\n",
    "            return_components (bool): if True, return predicted outcomes for control and treated separately.\n",
    "            verbose (bool): whether to output logs.\n",
    "        Returns:\n",
    "            np.array: predicted treatment effects.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        te = np.zeros((X.shape[0], len(self.t_groups)))\n",
    "        yhat_cs = {}\n",
    "        yhat_ts = {}\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            # Average the treatment effect predictions from the cross-fit models\n",
    "            models_tau = self.models_tau[group]\n",
    "            _te = np.r_[[model.predict(np.hstack([X, X])) for model in models_tau]].mean(axis=0)\n",
    "            te[:, i] = np.ravel(_te)\n",
    "            yhat_cs[group] = np.r_[\n",
    "                [model.predict(X) for model in self.models_mu_c]\n",
    "            ].mean(axis=0)\n",
    "            yhat_ts[group] = np.r_[\n",
    "                [model.predict(X) for model in self.models_mu_t[group]]\n",
    "            ].mean(axis=0)\n",
    "\n",
    "            if (y is not None) and (treatment is not None) and verbose:\n",
    "                mask = (treatment == group) | (treatment == self.control_name)\n",
    "                treatment_filt = treatment[mask]\n",
    "                y_filt = y[mask]\n",
    "                w = (treatment_filt == group).astype(int)\n",
    "                yhat = np.zeros_like(y_filt, dtype=float)\n",
    "                yhat[w == 0] = yhat_cs[group][mask][w == 0]\n",
    "                yhat[w == 1] = yhat_ts[group][mask][w == 1]\n",
    "                logger.info(\"Error metrics for group {}\".format(group))\n",
    "                regression_metrics(y_filt, yhat, w)\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, yhat_cs, yhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "            seed (int): random seed for cross-fitting\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p, seed)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "\n",
    "        check_p_conditions(p, self.t_groups)\n",
    "        if isinstance(p, (np.ndarray, pd.Series)):\n",
    "            treatment_name = self.t_groups[0]\n",
    "            p = {treatment_name: convert_pd_to_np(p)}\n",
    "        elif isinstance(p, dict):\n",
    "            p = {\n",
    "                treatment_name: convert_pd_to_np(_p) for treatment_name, _p in p.items()\n",
    "            }\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            models_mu_c_global = deepcopy(self.models_mu_c)\n",
    "            models_mu_t_global = deepcopy(self.models_mu_t)\n",
    "            models_tau_global = deepcopy(self.models_tau)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.models_mu_c = deepcopy(models_mu_c_global)\n",
    "            self.models_mu_t = deepcopy(models_mu_t_global)\n",
    "            self.models_tau = deepcopy(models_tau_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        seed=None,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            seed (int): random seed for cross-fitting\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            te, yhat_cs, yhat_ts = self.predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        else:\n",
    "            te, yhat_cs, yhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True, seed=seed\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            check_p_conditions(p, self.t_groups)\n",
    "        if isinstance(p, (np.ndarray, pd.Series)):\n",
    "            treatment_name = self.t_groups[0]\n",
    "            p = {treatment_name: convert_pd_to_np(p)}\n",
    "        elif isinstance(p, dict):\n",
    "            p = {\n",
    "                treatment_name: convert_pd_to_np(_p) for treatment_name, _p in p.items()\n",
    "            }\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            _ate = te[:, i].mean()\n",
    "\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "            prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "            yhat_c = yhat_cs[group][mask]\n",
    "            yhat_t = yhat_ts[group][mask]\n",
    "            y_filt = y[mask]\n",
    "\n",
    "            # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "            # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "            se = np.sqrt(\n",
    "                (\n",
    "                    (y_filt[w == 0] - yhat_c[w == 0]).var() / (1 - prob_treatment)\n",
    "                    + (y_filt[w == 1] - yhat_t[w == 1]).var() / prob_treatment\n",
    "                    + (yhat_t - yhat_c).var()\n",
    "                )\n",
    "                / y_filt.shape[0]\n",
    "            )\n",
    "\n",
    "            _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "            _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "            ate[i] = _ate\n",
    "            ate_lb[i] = _ate_lb\n",
    "            ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            models_mu_c_global = deepcopy(self.models_mu_c)\n",
    "            models_mu_t_global = deepcopy(self.models_mu_t)\n",
    "            models_tau_global = deepcopy(self.models_tau)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(\n",
    "                    X, treatment, y, p, size=bootstrap_size, seed=seed\n",
    "                )\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.models_mu_c = deepcopy(models_mu_c_global)\n",
    "            self.models_mu_t = deepcopy(models_mu_t_global)\n",
    "            self.models_tau = deepcopy(models_tau_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "\n",
    "class BasePDRRegressor(BasePDRLearner):\n",
    "    \"\"\"\n",
    "    A parent class for PDR-learner regressor classes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        control_outcome_learner=None,\n",
    "        treatment_outcome_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            control_outcome_learner=control_outcome_learner,\n",
    "            treatment_outcome_learner=treatment_outcome_learner,\n",
    "            treatment_effect_learner=treatment_effect_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Create the neural network\n",
    "def _build_mlp(input_dim, hidden_dims, output_dim=1, activation=nn.ReLU):\n",
    "    \"\"\"Builds a simple MLP.\"\"\"\n",
    "    layers = []\n",
    "    last_dim = input_dim\n",
    "    for hidden_dim in hidden_dims:\n",
    "        layers.append(nn.Linear(last_dim, hidden_dim))\n",
    "        layers.append(activation())\n",
    "        last_dim = hidden_dim\n",
    "    layers.append(nn.Linear(last_dim, output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Class for pairs\n",
    "class PairDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset to serve pairs of indices.\"\"\"\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs # List of tuples (idx_i, idx_j)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the pair of original indices\n",
    "        return self.pairs[idx]\n",
    "\n",
    "# PairNet implementation using PyTorch\n",
    "class PairNetTorch:\n",
    "    \"\"\"\n",
    "    PairNet implementation using PyTorch.\n",
    "\n",
    "    Uses a T-Learner architecture (separate networks for control and treatment)\n",
    "    trained jointly with the loss: ((y_i - y_j) - (mu(xi, ti) - mu(xj, tj)))^2\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dims: list = [64, 32],\n",
    "                 activation = nn.ReLU,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 weight_decay: float = 1e-4,\n",
    "                 epochs: int = 100,\n",
    "                 batch_size: int = 200,\n",
    "                 distance_threshold: float = np.inf,\n",
    "                 num_neighbours: int = 3, \n",
    "                 val_size: float = 0.0, # proportion used for validation, in [0,1]\n",
    "                 patience: int = 10,     # Early stopping patience\n",
    "                 verbose: int = 10,     # Print loss every `verbose` epochs\n",
    "                 random_state: int | None = None,\n",
    "                 device: str | None = None):\n",
    "        \"\"\"\n",
    "        Initialize the PairNet learner.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of features in X.\n",
    "            hidden_dims (list, optional): List of hidden layer sizes for mu_c and mu_t.\n",
    "                                          Defaults to [64, 32].\n",
    "            activation (torch.nn.Module, optional): Activation function. Defaults to nn.ReLU.\n",
    "            learning_rate (float, optional): Optimizer learning rate. Defaults to 1e-3.\n",
    "            weight_decay (float, optional): L2 regularization strength. Defaults to 1e-4.\n",
    "            epochs (int, optional): Number of training epochs. Defaults to 100.\n",
    "            batch_size (int, optional): Batch size for training pairs. Defaults to 200.\n",
    "            distance_threshold (float, optional): Max distance for pairing. Defaults to no max distance.\n",
    "            num_neighbours (int, optional): neighbours per sample in pairing. Defaults to 3.\n",
    "            val_size (float, optional): Fraction of data to use for validation/early stopping.\n",
    "                                        If 0, no validation/early stopping is used. Defaults to 0.\n",
    "            patience (int, optional): Early stopping patience (epochs without improvement). Defaults to 10.\n",
    "            verbose (int, optional): Print loss frequency (epochs). Defaults to 10.\n",
    "            random_state (int | None, optional): Random seed. Defaults to None.\n",
    "            device (str | None, optional): Device ('cpu', 'cuda'). Autodetects if None.\n",
    "        \"\"\"\n",
    "        \n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(random_state)\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.activation = activation\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.distance_threshold = distance_threshold\n",
    "        self.num_neighbours = num_neighbours\n",
    "        self.val_size = val_size\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Initialize models, optimizers, scaler\n",
    "        self.mu_c = _build_mlp(input_dim, hidden_dims, 1, activation).to(self.device)\n",
    "        self.mu_t = _build_mlp(input_dim, hidden_dims, 1, activation).to(self.device)\n",
    "\n",
    "        # Combine parameters for a single optimizer pass\n",
    "        all_params = list(self.mu_c.parameters()) + list(self.mu_t.parameters())\n",
    "        self.optimizer = optim.Adam(all_params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        self.scaler = StandardScaler() # Scale features for NN stability\n",
    "\n",
    "        # Placeholders for data and results\n",
    "        self.X_tensor = None\n",
    "        self.t_tensor = None\n",
    "        self.y_tensor = None\n",
    "        self.pairs_train = []\n",
    "        self.pairs_val = []\n",
    "        self.best_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "        self.best_mu_c_state = None\n",
    "        self.best_mu_t_state = None\n",
    "\n",
    "\n",
    "    def _softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Numerically stable softmax function.\"\"\"\n",
    "        if x.size == 0: \n",
    "            return np.array([])\n",
    "        return softmax(x)\n",
    "\n",
    "    def _create_pairs_torch(self, X_np: np.ndarray, t_np: np.ndarray) -> list[tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Creates pairs based on distance in the original feature space.\n",
    "        Returns a list of (idx_i, idx_j) tuples.\n",
    "        \"\"\"\n",
    "        n_samples = X_np.shape[0]\n",
    "        idx = np.arange(n_samples)\n",
    "        idx_c = idx[t_np == 0]\n",
    "        idx_t = idx[t_np == 1]\n",
    "\n",
    "        if len(idx_c) == 0 or len(idx_t) == 0:\n",
    "            print(\"Warning: No samples in control or treatment group for pairing.\")\n",
    "            return []\n",
    "\n",
    "        X_c = X_np[idx_c]\n",
    "        X_t = X_np[idx_t]\n",
    "\n",
    "        all_pairs = [] # List of (original_index_i, original_index_j)\n",
    "\n",
    "        # --- Function to process one direction (e.g., target=Treated, pool=Control) ---\n",
    "        def find_pairs_one_direction(target_indices, target_X, pool_indices, pool_X):\n",
    "            local_pairs = []\n",
    "            if len(target_X) == 0 or len(pool_X) == 0:\n",
    "                return local_pairs\n",
    "\n",
    "            distances = cdist(target_X, pool_X, metric='euclidean')\n",
    "\n",
    "            for i in range(len(target_X)):\n",
    "                dists_i = distances[i, :]\n",
    "                original_target_idx = target_indices[i]\n",
    "\n",
    "                # Softmax Sampling Logic\n",
    "                potential_neighbour_indices = np.where(dists_i <= self.distance_threshold)[0]\n",
    "                if len(potential_neighbour_indices) == 0: \n",
    "                    continue\n",
    "\n",
    "                valid_distances = dists_i[potential_neighbour_indices]\n",
    "                neg_distances = -valid_distances\n",
    "                probs = self._softmax(neg_distances)\n",
    "                if np.any(np.isnan(probs)) or not np.isclose(np.sum(probs), 1.0): \n",
    "                    probs = None\n",
    "\n",
    "                num_to_sample = min(self.num_neighbours, len(potential_neighbour_indices))\n",
    "                if num_to_sample == 0: \n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    sampled_local_indices = self.rng.choice(\n",
    "                        potential_neighbour_indices, size=num_to_sample, replace=True, p=probs\n",
    "                    )\n",
    "                except ValueError as e: # Catch potential issue with p summing slightly off 1.0\n",
    "                    print(f\"Warning: RNG choice error (likely prob sum issue) for sample {original_target_idx}: {e}. Sampling uniformly.\")\n",
    "                    sampled_local_indices = self.rng.choice(\n",
    "                        potential_neighbour_indices, size=num_to_sample, replace=True # Uniform sampling\n",
    "                    )\n",
    "\n",
    "                for local_idx in sampled_local_indices:\n",
    "                    original_pool_idx = pool_indices[local_idx]\n",
    "                    local_pairs.append((original_target_idx, original_pool_idx))\n",
    "\n",
    "            return local_pairs\n",
    "\n",
    "        # Find pairs for treated units (target=Treated, pool=Control)\n",
    "        pairs_tc = find_pairs_one_direction(idx_t, X_t, idx_c, X_c)\n",
    "        all_pairs.extend(pairs_tc)\n",
    "\n",
    "        # Find pairs for control units (target=Control, pool=Treated)\n",
    "        pairs_ct = find_pairs_one_direction(idx_c, X_c, idx_t, X_t)\n",
    "        all_pairs.extend(pairs_ct)\n",
    "\n",
    "        return all_pairs\n",
    "\n",
    "    def _get_predictions_for_pair(self, x_i, t_i, x_j, t_j):\n",
    "        \"\"\"Gets mu(x,t) predictions for both elements of pairs in a batch.\"\"\"\n",
    "        # Predict all i's and j's under both control and treatment scenarios\n",
    "        pred_i_c = self.mu_c(x_i)\n",
    "        pred_i_t = self.mu_t(x_i)\n",
    "        pred_j_c = self.mu_c(x_j)\n",
    "        pred_j_t = self.mu_t(x_j)\n",
    "\n",
    "        # Select the correct prediction based on actual treatment\n",
    "        t_i_mask = t_i.bool() # Convert to boolean mask\n",
    "        t_j_mask = t_j.bool()\n",
    "\n",
    "        pred_i = torch.where(t_i_mask, pred_i_t, pred_i_c)\n",
    "        pred_j = torch.where(t_j_mask, pred_j_t, pred_j_c)\n",
    "\n",
    "        return pred_i, pred_j\n",
    "\n",
    "    def _calculate_loss(self, idx_i_batch, idx_j_batch):\n",
    "        \"\"\"Calculates the pair loss for a batch of pairs.\"\"\"\n",
    "        # Retrieve data for the batch using indices\n",
    "        x_i = self.X_tensor[idx_i_batch]\n",
    "        t_i = self.t_tensor[idx_i_batch]\n",
    "        y_i = self.y_tensor[idx_i_batch]\n",
    "\n",
    "        x_j = self.X_tensor[idx_j_batch]\n",
    "        t_j = self.t_tensor[idx_j_batch]\n",
    "        y_j = self.y_tensor[idx_j_batch]\n",
    "\n",
    "        # Get predictions\n",
    "        pred_i, pred_j = self._get_predictions_for_pair(x_i, t_i, x_j, t_j)\n",
    "\n",
    "        # Calculate the pair loss\n",
    "        true_diff = y_i - y_j\n",
    "        pred_diff = pred_i - pred_j\n",
    "        loss = torch.mean((true_diff - pred_diff) ** 2) \n",
    "        return loss\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, treatment: pd.Series, y: pd.Series, total_time=None):\n",
    "        \"\"\"\n",
    "        Fit the PairNet model using the pair loss.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix.\n",
    "            treatment (pd.Series): Treatment assignment vector (0 or 1).\n",
    "            y (pd.Series): Outcome vector.\n",
    "            total_time (float): Time allowed to run.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Preprocessing and Data Setup\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        X_np = self.scaler.fit_transform(X).astype(np.float32)\n",
    "        t_np = treatment.astype(np.float32).reshape(-1, 1) # Ensure shape [n, 1]\n",
    "        y_np = y.astype(np.float32).reshape(-1, 1)       # Ensure shape [n, 1]\n",
    "\n",
    "        self.X_tensor = torch.tensor(X_np, dtype=torch.float32).to(self.device)\n",
    "        self.t_tensor = torch.tensor(t_np, dtype=torch.float32).to(self.device)\n",
    "        self.y_tensor = torch.tensor(y_np, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Create pairs\n",
    "        all_pairs = self._create_pairs_torch(X_np, t_np.flatten())\n",
    "        if not all_pairs:\n",
    "            raise ValueError(\"No pairs were created. Cannot train the model.\")\n",
    "\n",
    "        # Split pairs for Train/Validation (if val_size > 0)\n",
    "        if self.val_size > 0:\n",
    "            if len(all_pairs) < 2 / self.val_size or len(all_pairs) < 2 / (1 - self.val_size): # Ensure enough pairs for split\n",
    "                 print(\"Warning: Not enough pairs for validation split, disabling validation.\")\n",
    "                 self.val_size = 0\n",
    "                 self.pairs_train = all_pairs\n",
    "                 self.pairs_val = []\n",
    "            else:\n",
    "                 pairs_train_idx, pairs_val_idx = train_test_split(\n",
    "                    np.arange(len(all_pairs)), test_size=self.val_size, random_state=self.random_state\n",
    "                 )\n",
    "                 self.pairs_train = [all_pairs[i] for i in pairs_train_idx]\n",
    "                 self.pairs_val = [all_pairs[i] for i in pairs_val_idx]\n",
    "        else:\n",
    "             self.pairs_train = all_pairs\n",
    "             self.pairs_val = []\n",
    "\n",
    "        train_dataset = PairDataset(self.pairs_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        if self.pairs_val:\n",
    "            val_dataset = PairDataset(self.pairs_val)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "        # Training Loop\n",
    "        self.mu_c.train()\n",
    "        self.mu_t.train()\n",
    "        self.best_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_train_loss = 0.0\n",
    "            for i, (idx_i_batch, idx_j_batch) in enumerate(train_loader):\n",
    "                idx_i_batch = idx_i_batch.long()\n",
    "                idx_j_batch = idx_j_batch.long()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self._calculate_loss(idx_i_batch, idx_j_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            avg_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "\n",
    "            # Validation and Early Stopping\n",
    "            avg_epoch_val_loss = -1\n",
    "            if self.pairs_val:\n",
    "                self.mu_c.eval()\n",
    "                self.mu_t.eval()\n",
    "                epoch_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for idx_i_batch, idx_j_batch in val_loader:\n",
    "                        idx_i_batch = idx_i_batch.long()\n",
    "                        idx_j_batch = idx_j_batch.long()\n",
    "                        loss = self._calculate_loss(idx_i_batch, idx_j_batch)\n",
    "                        epoch_val_loss += loss.item()\n",
    "                avg_epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "\n",
    "                if avg_epoch_val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = avg_epoch_val_loss\n",
    "                    self.epochs_no_improve = 0\n",
    "                    # Store the best model state\n",
    "                    self.best_mu_c_state = self.mu_c.state_dict()\n",
    "                    self.best_mu_t_state = self.mu_t.state_dict()\n",
    "                else:\n",
    "                    self.epochs_no_improve += 1\n",
    "\n",
    "                # Set back to train mode\n",
    "                self.mu_c.train()\n",
    "                self.mu_t.train()\n",
    "\n",
    "            if self.verbose > 0 and (epoch + 1) % self.verbose == 0:\n",
    "                if self.pairs_val:\n",
    "                    print(f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_epoch_val_loss:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "            if self.val_size > 0 and self.epochs_no_improve >= self.patience:\n",
    "                #print(f\"Early stopping triggered after epoch {epoch+1}. Best Val Loss: {self.best_val_loss:.4f}\")\n",
    "                # Load the best model state before breaking\n",
    "                if self.best_mu_c_state and self.best_mu_t_state:\n",
    "                    self.mu_c.load_state_dict(self.best_mu_c_state)\n",
    "                    self.mu_t.load_state_dict(self.best_mu_t_state)\n",
    "                break\n",
    "            \n",
    "            # Break if used time allowed\n",
    "            elapsed_time = time.perf_counter() - start_time\n",
    "            if elapsed_time > total_time:\n",
    "                break\n",
    "\n",
    "        # If not early stopping or if early stopping happened on last epoch,\n",
    "        # potentially load the best model state if validation was used.\n",
    "        if self.val_size > 0 and self.best_mu_c_state and self.best_mu_t_state:\n",
    "             self.mu_c.load_state_dict(self.best_mu_c_state)\n",
    "             self.mu_t.load_state_dict(self.best_mu_t_state)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, return_components: bool = False) -> np.ndarray | tuple:\n",
    "        \"\"\"\n",
    "        Predict Conditional Average Treatment Effect (CATE).\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix for prediction.\n",
    "            return_components (bool, optional): If True, return tuple of\n",
    "                                                (cate, y_pred_c, y_pred_t).\n",
    "                                                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray or tuple: Predicted CATE = mu_t(X) - mu_c(X)\n",
    "                                 (or tuple if return_components is True).\n",
    "        \"\"\"\n",
    "        if self.X_tensor is None: # Check if fit has been called\n",
    "            raise RuntimeError(\"Model has not been fitted yet. Call fit() first.\")\n",
    "\n",
    "        self.mu_c.eval()\n",
    "        self.mu_t.eval()\n",
    "\n",
    "        X = convert_pd_to_np(X)\n",
    "        X_np = self.scaler.transform(X).astype(np.float32) # Scale data\n",
    "        X_pred_tensor = torch.tensor(X_np, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_c = self.mu_c(X_pred_tensor)\n",
    "            y_pred_t = self.mu_t(X_pred_tensor)\n",
    "\n",
    "        cate_pred = y_pred_t - y_pred_c\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        cate_pred_np = cate_pred.cpu().numpy()\n",
    "        if return_components:\n",
    "            y_pred_c_np = y_pred_c.cpu().numpy()\n",
    "            y_pred_t_np = y_pred_t.cpu().numpy()\n",
    "            return cate_pred_np, y_pred_c_np, y_pred_t_np\n",
    "        else:\n",
    "            return cate_pred_np\n",
    "\n",
    "    def estimate_ate(self, X: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix for ATE estimation.\n",
    "\n",
    "        Returns:\n",
    "            float: Estimated ATE.\n",
    "        \"\"\"\n",
    "        cate_pred = self.predict(X, return_components=False)\n",
    "        # Average the predictions\n",
    "        return float(np.mean(cate_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "class LassoSplineLearner:\n",
    "    def __init__(self, df_spline=7, cv=5):\n",
    "        \"\"\"\n",
    "        LASSO model with natural spline and pairwise interactions.\n",
    "        \n",
    "        :param df_spline: Degrees of freedom for the natural spline.\n",
    "        :param cv: Number of cross-validation folds for LASSO.\n",
    "        \"\"\"\n",
    "        self.df_spline = df_spline\n",
    "        self.cv = cv\n",
    "        self.lasso = None  # Model will be assigned after fitting\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.df_spline, self.cv\n",
    "    \n",
    "    def _transform_features(self, X):\n",
    "        \"\"\"Apply natural spline expansion and pairwise interactions.\"\"\"\n",
    "        df = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(X.shape[1])])\n",
    "        # Create spline transformations for each feature individually\n",
    "        spline_terms = []\n",
    "        for i in range(X.shape[1]):\n",
    "            spline_terms.append(f\"bs(X{i}, df={self.df_spline}, include_intercept=False)\")\n",
    "        \n",
    "        formula = \" + \".join(spline_terms)  # Combine all spline transformations\n",
    "        spline_basis = dmatrix(formula, data=df, return_type='dataframe')\n",
    "\n",
    "        # Apply pairwise interactions\n",
    "        poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=True, )\n",
    "        return poly.fit_transform(spline_basis)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit LASSO on transformed features.\"\"\"\n",
    "        X_transformed = self._transform_features(X)\n",
    "        self.lasso = Lasso(random_state=123, ).fit(X_transformed, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using trained LASSO model.\"\"\"\n",
    "        if self.lasso is None:\n",
    "            raise ValueError(\"Model is not trained. Call `.fit()` first.\")\n",
    "        X_transformed = self._transform_features(X)\n",
    "        return self.lasso.predict(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model, learner):\n",
    "    learner_dict = {\n",
    "        'S-Learner': BaseSRegressor(learner),\n",
    "        'T-Learner': BaseTRegressor(learner),\n",
    "        'X-Learner': BaseXRegressor(learner),\n",
    "        'RT-Learner': BaseRTRegressor(learner),\n",
    "        'P-Learner': BasePRegressor(learner),\n",
    "        'RP-Learner': BaseRPRegressor(learner)\n",
    "    }\n",
    "    return learner_dict[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairnet_nn(input_dim, hidden_dims, epochs):\n",
    "    pairnet_nn = PairNetTorch(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=hidden_dims,\n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0001,\n",
    "        epochs=epochs,\n",
    "        batch_size=200,\n",
    "        distance_threshold=np.inf, # Use max neighbours for all samples\n",
    "        num_neighbours=3,\n",
    "        val_size=0.1, # Use 10% of pairs for validation/early stopping\n",
    "        patience=10,\n",
    "        verbose=epochs+1, # No verbosity\n",
    "    )\n",
    "    return pairnet_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(n_list, p_list, s_list, m_list, learner_dict, num_iter,\n",
    "                    propensity_learner=None):\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    for i in tqdm(range(num_iter)):\n",
    "\n",
    "        for n, p, s, m in product(n_list, p_list, s_list, m_list):\n",
    "            \n",
    "            # No limit on time if P-learner not run\n",
    "            pairnet_time=np.inf\n",
    "\n",
    "            # X, W, y, tau = generate_data_setup_a(n*2, p, s)\n",
    "\n",
    "            y, X, W, tau, _, _ = synthetic_data(mode=m, n=n*2, p=p, sigma=s)\n",
    "            X_train, X_test, W_train, _, y_train, _, _, tau_test = train_test_split(\n",
    "                X, W, y, tau, test_size=0.5, random_state=111)\n",
    "\n",
    "            if propensity_learner is not None:\n",
    "                em = clone(propensity_learner)\n",
    "                em.fit(X_train, W_train)\n",
    "                e_hat_train = cross_val_predict(em, X_train, W_train, method='predict_proba')[:, 1]\n",
    "                e_hat_test = em.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            for learner in ['P-learner', 'PairNet']:\n",
    "\n",
    "                start_time = time.perf_counter()\n",
    "\n",
    "                if learner != 'PairNet':\n",
    "                    model = deepcopy(learner_dict[learner])\n",
    "                    model.fit(X=X_train, treatment=W_train, y=y_train, p=e_hat_train)\n",
    "                    hat_tau = model.predict(X_test, p=e_hat_test)\n",
    "                else:\n",
    "                    model = create_pairnet_nn(X_train.shape[1], (16,16), 1000)\n",
    "                    model.fit(X_train, W_train, y_train, total_time=pairnet_time)\n",
    "                    hat_tau = model.predict(X_test)\n",
    "\n",
    "                pehe = mean_squared_error(tau_test, hat_tau)\n",
    "\n",
    "                elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "                # If P-learner run, limit time to train PairNet\n",
    "                if learner == 'P-learner':\n",
    "                    pairnet_time = elapsed_time\n",
    "\n",
    "                result_list.append([n, p, s, m, learner, pehe, elapsed_time])  \n",
    "\n",
    "    cols = ['num_samples', 'num_features', 'sigma', 'sim_mode', 'learner', 'pehe', 'time']\n",
    "    df_res = pd.DataFrame(result_list, columns=cols)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:23<00:00,  5.63s/it]\n"
     ]
    }
   ],
   "source": [
    "# Simulation params from Nie and Wager (2020)\n",
    "n_list = [1000]\n",
    "p_list = [10]\n",
    "s_list = [0, 1, 4]\n",
    "m_list = [1,2,3,4]\n",
    "num_iter = 100\n",
    "\n",
    "learner=RandomForestRegressor(max_depth=10)\n",
    "learner = MLPRegressor((100,100), max_iter=100, early_stopping=True, validation_fraction=0.1)\n",
    "#learner=LassoSplineLearner()\n",
    "#learner = XGBRegressor(max_depth=10)\n",
    "\n",
    "learner_dict = {\n",
    "    #'S-learner': BaseSRegressor(learner),\n",
    "    #'T-learner': BaseTRegressor(learner),\n",
    "    #'X-learner': BaseXRegressor(learner),\n",
    "    #'R-learner': BaseRRegressor(learner),\n",
    "    #'DR-learner': BaseDRRegressor(learner),\n",
    "    #'PDR-learner': BasePDRRegressor(learner),\n",
    "    #'RT-learner': BaseRTRegressor(learner=learner),\n",
    "    'P-learner': BasePRegressor(learner),\n",
    "    #'RP-learner': BaseRPRegressor(learner=learner),\n",
    "    'PairNet': ''\n",
    "}\n",
    "\n",
    "propensity_learner = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "df_res_nn_p = run_experiments(n_list, p_list, s_list, m_list, learner_dict, num_iter, propensity_learner=propensity_learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d9e30_row0_col6, #T_d9e30_row1_col6, #T_d9e30_row2_col7, #T_d9e30_row3_col6, #T_d9e30_row4_col6, #T_d9e30_row5_col7, #T_d9e30_row6_col6, #T_d9e30_row7_col7, #T_d9e30_row8_col7, #T_d9e30_row9_col6, #T_d9e30_row10_col6, #T_d9e30_row11_col7 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d9e30\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d9e30_level0_col0\" class=\"col_heading level0 col0\" >num_samples</th>\n",
       "      <th id=\"T_d9e30_level0_col1\" class=\"col_heading level0 col1\" >num_features</th>\n",
       "      <th id=\"T_d9e30_level0_col2\" class=\"col_heading level0 col2\" >sigma</th>\n",
       "      <th id=\"T_d9e30_level0_col3\" class=\"col_heading level0 col3\" >sim_mode</th>\n",
       "      <th id=\"T_d9e30_level0_col4\" class=\"col_heading level0 col4\" colspan=\"2\">mean</th>\n",
       "      <th id=\"T_d9e30_level0_col6\" class=\"col_heading level0 col6\" colspan=\"2\">sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"blank level1\" >&nbsp;</th>\n",
       "      <th id=\"T_d9e30_level1_col0\" class=\"col_heading level1 col0\" ></th>\n",
       "      <th id=\"T_d9e30_level1_col1\" class=\"col_heading level1 col1\" ></th>\n",
       "      <th id=\"T_d9e30_level1_col2\" class=\"col_heading level1 col2\" ></th>\n",
       "      <th id=\"T_d9e30_level1_col3\" class=\"col_heading level1 col3\" ></th>\n",
       "      <th id=\"T_d9e30_level1_col4\" class=\"col_heading level1 col4\" colspan=\"2\">pehe</th>\n",
       "      <th id=\"T_d9e30_level1_col6\" class=\"col_heading level1 col6\" colspan=\"2\">pehe</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level2\" >learner</th>\n",
       "      <th id=\"T_d9e30_level2_col0\" class=\"col_heading level2 col0\" ></th>\n",
       "      <th id=\"T_d9e30_level2_col1\" class=\"col_heading level2 col1\" ></th>\n",
       "      <th id=\"T_d9e30_level2_col2\" class=\"col_heading level2 col2\" ></th>\n",
       "      <th id=\"T_d9e30_level2_col3\" class=\"col_heading level2 col3\" ></th>\n",
       "      <th id=\"T_d9e30_level2_col4\" class=\"col_heading level2 col4\" >P-learner</th>\n",
       "      <th id=\"T_d9e30_level2_col5\" class=\"col_heading level2 col5\" >PairNet</th>\n",
       "      <th id=\"T_d9e30_level2_col6\" class=\"col_heading level2 col6\" >P-learner</th>\n",
       "      <th id=\"T_d9e30_level2_col7\" class=\"col_heading level2 col7\" >PairNet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d9e30_row0_col0\" class=\"data row0 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row0_col1\" class=\"data row0 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_d9e30_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "      <td id=\"T_d9e30_row0_col4\" class=\"data row0 col4\" >0.034</td>\n",
       "      <td id=\"T_d9e30_row0_col5\" class=\"data row0 col5\" >0.154</td>\n",
       "      <td id=\"T_d9e30_row0_col6\" class=\"data row0 col6\" >0.001</td>\n",
       "      <td id=\"T_d9e30_row0_col7\" class=\"data row0 col7\" >0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row1\" class=\"row_heading level0 row1\" >4</th>\n",
       "      <td id=\"T_d9e30_row1_col0\" class=\"data row1 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row1_col1\" class=\"data row1 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "      <td id=\"T_d9e30_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "      <td id=\"T_d9e30_row1_col4\" class=\"data row1 col4\" >0.182</td>\n",
       "      <td id=\"T_d9e30_row1_col5\" class=\"data row1 col5\" >0.180</td>\n",
       "      <td id=\"T_d9e30_row1_col6\" class=\"data row1 col6\" >0.008</td>\n",
       "      <td id=\"T_d9e30_row1_col7\" class=\"data row1 col7\" >0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row2\" class=\"row_heading level0 row2\" >8</th>\n",
       "      <td id=\"T_d9e30_row2_col0\" class=\"data row2 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row2_col1\" class=\"data row2 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row2_col2\" class=\"data row2 col2\" >4</td>\n",
       "      <td id=\"T_d9e30_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "      <td id=\"T_d9e30_row2_col4\" class=\"data row2 col4\" >1.346</td>\n",
       "      <td id=\"T_d9e30_row2_col5\" class=\"data row2 col5\" >0.243</td>\n",
       "      <td id=\"T_d9e30_row2_col6\" class=\"data row2 col6\" >0.108</td>\n",
       "      <td id=\"T_d9e30_row2_col7\" class=\"data row2 col7\" >0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row3\" class=\"row_heading level0 row3\" >1</th>\n",
       "      <td id=\"T_d9e30_row3_col0\" class=\"data row3 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row3_col1\" class=\"data row3 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_d9e30_row3_col3\" class=\"data row3 col3\" >2</td>\n",
       "      <td id=\"T_d9e30_row3_col4\" class=\"data row3 col4\" >0.159</td>\n",
       "      <td id=\"T_d9e30_row3_col5\" class=\"data row3 col5\" >0.727</td>\n",
       "      <td id=\"T_d9e30_row3_col6\" class=\"data row3 col6\" >0.004</td>\n",
       "      <td id=\"T_d9e30_row3_col7\" class=\"data row3 col7\" >0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row4\" class=\"row_heading level0 row4\" >5</th>\n",
       "      <td id=\"T_d9e30_row4_col0\" class=\"data row4 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row4_col1\" class=\"data row4 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "      <td id=\"T_d9e30_row4_col3\" class=\"data row4 col3\" >2</td>\n",
       "      <td id=\"T_d9e30_row4_col4\" class=\"data row4 col4\" >0.634</td>\n",
       "      <td id=\"T_d9e30_row4_col5\" class=\"data row4 col5\" >0.862</td>\n",
       "      <td id=\"T_d9e30_row4_col6\" class=\"data row4 col6\" >0.016</td>\n",
       "      <td id=\"T_d9e30_row4_col7\" class=\"data row4 col7\" >0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row5\" class=\"row_heading level0 row5\" >9</th>\n",
       "      <td id=\"T_d9e30_row5_col0\" class=\"data row5 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row5_col1\" class=\"data row5 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row5_col2\" class=\"data row5 col2\" >4</td>\n",
       "      <td id=\"T_d9e30_row5_col3\" class=\"data row5 col3\" >2</td>\n",
       "      <td id=\"T_d9e30_row5_col4\" class=\"data row5 col4\" >5.272</td>\n",
       "      <td id=\"T_d9e30_row5_col5\" class=\"data row5 col5\" >1.218</td>\n",
       "      <td id=\"T_d9e30_row5_col6\" class=\"data row5 col6\" >0.235</td>\n",
       "      <td id=\"T_d9e30_row5_col7\" class=\"data row5 col7\" >0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row6\" class=\"row_heading level0 row6\" >2</th>\n",
       "      <td id=\"T_d9e30_row6_col0\" class=\"data row6 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row6_col1\" class=\"data row6 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_d9e30_row6_col3\" class=\"data row6 col3\" >3</td>\n",
       "      <td id=\"T_d9e30_row6_col4\" class=\"data row6 col4\" >0.309</td>\n",
       "      <td id=\"T_d9e30_row6_col5\" class=\"data row6 col5\" >1.229</td>\n",
       "      <td id=\"T_d9e30_row6_col6\" class=\"data row6 col6\" >0.030</td>\n",
       "      <td id=\"T_d9e30_row6_col7\" class=\"data row6 col7\" >0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row7\" class=\"row_heading level0 row7\" >6</th>\n",
       "      <td id=\"T_d9e30_row7_col0\" class=\"data row7 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row7_col1\" class=\"data row7 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row7_col2\" class=\"data row7 col2\" >1</td>\n",
       "      <td id=\"T_d9e30_row7_col3\" class=\"data row7 col3\" >3</td>\n",
       "      <td id=\"T_d9e30_row7_col4\" class=\"data row7 col4\" >1.049</td>\n",
       "      <td id=\"T_d9e30_row7_col5\" class=\"data row7 col5\" >1.302</td>\n",
       "      <td id=\"T_d9e30_row7_col6\" class=\"data row7 col6\" >0.044</td>\n",
       "      <td id=\"T_d9e30_row7_col7\" class=\"data row7 col7\" >0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row8\" class=\"row_heading level0 row8\" >10</th>\n",
       "      <td id=\"T_d9e30_row8_col0\" class=\"data row8 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row8_col1\" class=\"data row8 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row8_col2\" class=\"data row8 col2\" >4</td>\n",
       "      <td id=\"T_d9e30_row8_col3\" class=\"data row8 col3\" >3</td>\n",
       "      <td id=\"T_d9e30_row8_col4\" class=\"data row8 col4\" >6.061</td>\n",
       "      <td id=\"T_d9e30_row8_col5\" class=\"data row8 col5\" >1.346</td>\n",
       "      <td id=\"T_d9e30_row8_col6\" class=\"data row8 col6\" >0.264</td>\n",
       "      <td id=\"T_d9e30_row8_col7\" class=\"data row8 col7\" >0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row9\" class=\"row_heading level0 row9\" >3</th>\n",
       "      <td id=\"T_d9e30_row9_col0\" class=\"data row9 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row9_col1\" class=\"data row9 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_d9e30_row9_col3\" class=\"data row9 col3\" >4</td>\n",
       "      <td id=\"T_d9e30_row9_col4\" class=\"data row9 col4\" >0.039</td>\n",
       "      <td id=\"T_d9e30_row9_col5\" class=\"data row9 col5\" >0.421</td>\n",
       "      <td id=\"T_d9e30_row9_col6\" class=\"data row9 col6\" >0.002</td>\n",
       "      <td id=\"T_d9e30_row9_col7\" class=\"data row9 col7\" >0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row10\" class=\"row_heading level0 row10\" >7</th>\n",
       "      <td id=\"T_d9e30_row10_col0\" class=\"data row10 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row10_col1\" class=\"data row10 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row10_col2\" class=\"data row10 col2\" >1</td>\n",
       "      <td id=\"T_d9e30_row10_col3\" class=\"data row10 col3\" >4</td>\n",
       "      <td id=\"T_d9e30_row10_col4\" class=\"data row10 col4\" >0.752</td>\n",
       "      <td id=\"T_d9e30_row10_col5\" class=\"data row10 col5\" >0.593</td>\n",
       "      <td id=\"T_d9e30_row10_col6\" class=\"data row10 col6\" >0.017</td>\n",
       "      <td id=\"T_d9e30_row10_col7\" class=\"data row10 col7\" >0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9e30_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_d9e30_row11_col0\" class=\"data row11 col0\" >1000</td>\n",
       "      <td id=\"T_d9e30_row11_col1\" class=\"data row11 col1\" >10</td>\n",
       "      <td id=\"T_d9e30_row11_col2\" class=\"data row11 col2\" >4</td>\n",
       "      <td id=\"T_d9e30_row11_col3\" class=\"data row11 col3\" >4</td>\n",
       "      <td id=\"T_d9e30_row11_col4\" class=\"data row11 col4\" >9.019</td>\n",
       "      <td id=\"T_d9e30_row11_col5\" class=\"data row11 col5\" >1.413</td>\n",
       "      <td id=\"T_d9e30_row11_col6\" class=\"data row11 col6\" >0.292</td>\n",
       "      <td id=\"T_d9e30_row11_col7\" class=\"data row11 col7\" >0.051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17aa74590>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot table to restructure the dataframe\n",
    "df_pivot = df_res_nn_p.pivot_table(\n",
    "    index=['num_samples', 'num_features', 'sigma', 'sim_mode',],  # Grouping columns\n",
    "    columns=\"learner\",          # Learner categories become new columns\n",
    "    values=[\"pehe\"],              # MSE values\n",
    "    aggfunc=[\"mean\", 'sem']              # Averaging the MSE values\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns if needed\n",
    "df_pivot.columns.name = None  # Remove the automatic column name\n",
    "\n",
    "# Function to apply bold formatting\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return [\"font-weight: bold\" if v else \"\" for v in is_min]\n",
    "\n",
    "# Apply formatting to only the learner columns\n",
    "learner_columns = df_pivot.columns[4:]  # Excluding 'n', 'd', 'sigma'\n",
    "styled_df = df_pivot.sort_values(['sim_mode', 'num_features','sigma']).style.format(precision=3).apply(highlight_min, subset=learner_columns, axis=1)\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{lrrrrrrrr}\\n & num_samples & num_features & sigma & sim_mode & \\\\multicolumn{2}{r}{mean} & \\\\multicolumn{2}{r}{sem} \\\\\\\\\\n &  &  &  &  & \\\\multicolumn{2}{r}{pehe} & \\\\multicolumn{2}{r}{pehe} \\\\\\\\\\nlearner &  &  &  &  & P-learner & PairNet & P-learner & PairNet \\\\\\\\\\n0 & 1000 & 10 & 0 & 1 & 0.034 & 0.154 & \\\\font-weightbold 0.001 & 0.004 \\\\\\\\\\n4 & 1000 & 10 & 1 & 1 & 0.182 & 0.180 & \\\\font-weightbold 0.008 & 0.010 \\\\\\\\\\n8 & 1000 & 10 & 4 & 1 & 1.346 & 0.243 & 0.108 & \\\\font-weightbold 0.021 \\\\\\\\\\n1 & 1000 & 10 & 0 & 2 & 0.159 & 0.727 & \\\\font-weightbold 0.004 & 0.026 \\\\\\\\\\n5 & 1000 & 10 & 1 & 2 & 0.634 & 0.862 & \\\\font-weightbold 0.016 & 0.028 \\\\\\\\\\n9 & 1000 & 10 & 4 & 2 & 5.272 & 1.218 & 0.235 & \\\\font-weightbold 0.032 \\\\\\\\\\n2 & 1000 & 10 & 0 & 3 & 0.309 & 1.229 & \\\\font-weightbold 0.030 & 0.033 \\\\\\\\\\n6 & 1000 & 10 & 1 & 3 & 1.049 & 1.302 & 0.044 & \\\\font-weightbold 0.034 \\\\\\\\\\n10 & 1000 & 10 & 4 & 3 & 6.061 & 1.346 & 0.264 & \\\\font-weightbold 0.055 \\\\\\\\\\n3 & 1000 & 10 & 0 & 4 & 0.039 & 0.421 & \\\\font-weightbold 0.002 & 0.023 \\\\\\\\\\n7 & 1000 & 10 & 1 & 4 & 0.752 & 0.593 & \\\\font-weightbold 0.017 & 0.024 \\\\\\\\\\n11 & 1000 & 10 & 4 & 4 & 9.019 & 1.413 & 0.292 & \\\\font-weightbold 0.051 \\\\\\\\\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styled_df.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generation_descs = {\n",
    "    1: 'Setup 1',\n",
    "    2: 'Setup 2',\n",
    "    3: 'Setup 3',\n",
    "    4: 'Setup 4'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_res_nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m axs \u001b[38;5;241m=\u001b[39m axs\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m), m_list):\n\u001b[0;32m----> 5\u001b[0m     sns\u001b[38;5;241m.\u001b[39mboxplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearner\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpehe\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39m\u001b[43mdf_res_nn\u001b[49m\u001b[38;5;241m.\u001b[39mloc[(df_res_nn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msim_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m m) \u001b[38;5;241m&\u001b[39m (df_res_nn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m      6\u001b[0m                                                           (df_res_nn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearner\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRT-Learner\u001b[39m\u001b[38;5;124m'\u001b[39m)], linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, showfliers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m      7\u001b[0m                                                           ax\u001b[38;5;241m=\u001b[39maxs[i], palette\u001b[38;5;241m=\u001b[39mpalette)\n\u001b[1;32m      8\u001b[0m     axs[i]\u001b[38;5;241m.\u001b[39mtitle\u001b[38;5;241m.\u001b[39mset_text(data_generation_descs[m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (RF)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     axs[i]\u001b[38;5;241m.\u001b[39mset_ylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime (s)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_res_nn' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, m in zip(range(4), m_list):\n",
    "    sns.boxplot(x='learner', y='pehe', data=df_res_nn.loc[(df_res_nn['sim_mode'] == m) & (df_res_nn['sigma'] == 1) & \n",
    "                                                          (df_res_nn['learner'] != 'RT-Learner')], linewidth=1, showfliers=False, \n",
    "                                                          ax=axs[i], palette=palette)\n",
    "    axs[i].title.set_text(data_generation_descs[m] + r' (RF)')\n",
    "    axs[i].set_ylabel('time (s)')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(labelsize=12)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i, m in zip(range(2), m_list):\n",
    "    sns.boxplot(x='learner', y='pehe', data=df_res_nn.loc[(df_res_nn['sim_mode'] == m) & (df_res_nn['sigma'] == 4) & \n",
    "                                                          (df_res_nn['learner'] != 'RT-Learner')], linewidth=1, showfliers=False, \n",
    "                                                          ax=axs[i], palette=palette)\n",
    "    axs[i].title.set_text(data_generation_descs[m] + r' (NN, $\\sigma=4$)')\n",
    "    axs[i].set_ylabel('PEHE')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(labelsize=12)\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
