{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from causalml.inference.meta import BaseXRegressor, BaseRRegressor, BaseSRegressor, BaseTRegressor, BaseDRRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.utils.extmath import cartesian\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from causalml.inference.meta.base import BaseLearner\n",
    "from causalml.inference.meta.utils import (\n",
    "    check_treatment_vector,\n",
    "    check_p_conditions,\n",
    "    convert_pd_to_np,\n",
    ")\n",
    "from causalml.metrics import regression_metrics\n",
    "from causalml.propensity import compute_propensity_score\n",
    "\n",
    "logger = logging.getLogger(\"causalml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRTLearner(BaseLearner):\n",
    "    \"\"\"A parent class for RT-learner regressor classes.\n",
    "\n",
    "    A RT-learner estimates treatment effects with three machine learning models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        control_effect_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a RT-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (outcome_learner is not None)\n",
    "            and (control_effect_learner is not None)\n",
    "            and (treatment_effect_learner is not None)\n",
    "        )\n",
    "\n",
    "        if outcome_learner is None:\n",
    "            self.model_mu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu = outcome_learner\n",
    "\n",
    "        if control_effect_learner is None:\n",
    "            self.model_tau_c = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau_c = control_effect_learner\n",
    "\n",
    "        if treatment_effect_learner is None:\n",
    "            self.model_tau_t = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau_t = treatment_effect_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(outcome_learner={},\\n\"\n",
    "            \"\\tcontrol_effect_learner={},\\n\"\n",
    "            \"\\ttreatment_effect_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu.__repr__(),\n",
    "                self.model_tau_c.__repr__(),\n",
    "                self.model_tau_t.__repr__(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        if p is None:\n",
    "            self._set_propensity_models(X=X, treatment=treatment, y=y)\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "        self.models_tau_c = {\n",
    "            group: deepcopy(self.model_tau_c) for group in self.t_groups\n",
    "        }\n",
    "        self.models_tau_t = {\n",
    "            group: deepcopy(self.model_tau_t) for group in self.t_groups\n",
    "        }\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_mu.fit(X, y)\n",
    "\n",
    "        for group in self.t_groups:\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            X_filt = X[mask]\n",
    "            y_filt = y[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "\n",
    "            # Calculate variances and treatment effects\n",
    "            var_c = (\n",
    "                y_filt[w == 0] - self.model_mu.predict(X_filt[w == 0])\n",
    "            ).var()\n",
    "            self.vars_c[group] = var_c\n",
    "            var_t = (\n",
    "                y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1])\n",
    "            ).var()\n",
    "            self.vars_t[group] = var_t\n",
    "\n",
    "            # Train treatment models\n",
    "            d_c = (self.model_mu.predict(X_filt[w == 0]) - y_filt[w == 0])\n",
    "            d_t = (y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1]))\n",
    "            self.models_tau_c[group].fit(X_filt[w == 0], d_c)\n",
    "            self.models_tau_t[group].fit(X_filt[w == 1], d_t)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            logger.info(\"Generating propensity score\")\n",
    "            p = dict()\n",
    "            for group in self.t_groups:\n",
    "                p_model = self.propensity_model[group]\n",
    "                p[group] = p_model.predict(X)\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "        dhat_cs = {}\n",
    "        dhat_ts = {}\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            model_tau_c = self.models_tau_c[group]\n",
    "            model_tau_t = self.models_tau_t[group]\n",
    "            dhat_cs[group] = model_tau_c.predict(X)\n",
    "            dhat_ts[group] = model_tau_t.predict(X)\n",
    "\n",
    "            _te = (dhat_cs[group] + dhat_ts[group]).reshape(\n",
    "                -1, 1\n",
    "            )\n",
    "            te[:, i] = np.ravel(_te)\n",
    "\n",
    "            if (y is not None) and (treatment is not None) and verbose:\n",
    "                mask = (treatment == group) | (treatment == self.control_name)\n",
    "                treatment_filt = treatment[mask]\n",
    "                X_filt = X[mask]\n",
    "                y_filt = y[mask]\n",
    "                w = (treatment_filt == group).astype(int)\n",
    "\n",
    "                yhat = np.zeros_like(y, dtype=float)\n",
    "                yhat = self.model_mu.predict(X)\n",
    "\n",
    "                logger.info(\"Error metrics for group {}\".format(group))\n",
    "                regression_metrics(y, yhat, w)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, dhat_cs, dhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, p=p, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            models_tau_c_global = deepcopy(self.models_tau_c)\n",
    "            models_tau_t_global = deepcopy(self.models_tau_t)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_tau_c = deepcopy(models_tau_c_global)\n",
    "            self.models_tau_t = deepcopy(models_tau_t_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=self.propensity, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=p, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, dhat_cs, dhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            _ate = te[:, i].mean()\n",
    "\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "            prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "            dhat_c = dhat_cs[group][mask]\n",
    "            dhat_t = dhat_ts[group][mask]\n",
    "            p_filt = p[group][mask]\n",
    "\n",
    "            # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "            # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "            se = np.sqrt(\n",
    "                (\n",
    "                    self.vars_t[group] / prob_treatment\n",
    "                    + self.vars_c[group] / (1 - prob_treatment)\n",
    "                    + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "                )\n",
    "                / w.shape[0]\n",
    "            )\n",
    "\n",
    "            _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "            _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "            ate[i] = _ate\n",
    "            ate_lb[i] = _ate_lb\n",
    "            ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            models_tau_c_global = deepcopy(self.models_tau_c)\n",
    "            models_tau_t_global = deepcopy(self.models_tau_t)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_tau_c = deepcopy(models_tau_c_global)\n",
    "            self.models_tau_t = deepcopy(models_tau_t_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "class BaseRTRegressor(BaseRTLearner):\n",
    "    \"\"\"\n",
    "    A parent class for RT-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        control_effect_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize an RT-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            outcome_learner=outcome_learner,\n",
    "            control_effect_learner=control_effect_learner,\n",
    "            treatment_effect_learner=treatment_effect_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePLearner(BaseLearner):\n",
    "    \"\"\"A parent class for P-learner regressor classes.\n",
    "\n",
    "    A P-learner estimates treatment effects with one machine learning model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a P-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            control_effect_learner (optional): a model to estimate treatment effects in the control group\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None)\n",
    "\n",
    "        self.model_nu = deepcopy(learner)\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(pairwise_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_nu.__repr__(),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def pair_data(self, X, treatment, y, num_samples):\n",
    "\n",
    "        # Split T=0 and T=1\n",
    "        X0 = X[treatment == 0]\n",
    "        Y0 = y[treatment == 0]\n",
    "        X1 = X[treatment == 1]\n",
    "        Y1 = y[treatment == 1]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X1_scaled = scaler.fit_transform(X1)\n",
    "        X0_scaled = scaler.fit_transform(X0)\n",
    "\n",
    "        # Build KD-tree from Y\n",
    "        tree = cKDTree(X1_scaled)\n",
    "\n",
    "        # Query for k nearest neighbours\n",
    "        distances, indices = tree.query(X0_scaled, k=num_samples)\n",
    "        # Randomly sample rows from X_treated for each row in X_control\n",
    "        random_indices = np.random.randint(0, len(X1), (len(X0), num_samples))\n",
    "        \n",
    "        # Expand T=0 data to match shape\n",
    "        X0_expanded = np.repeat(X0, num_samples, axis=0)  # Repeat X0\n",
    "        Y0_expanded = np.repeat(Y0, num_samples, axis=0)  # Repeat Y0\n",
    "\n",
    "        # Get corresponding T=1 data (using the random indices)\n",
    "        X1_sampled = X1[indices.flatten()]\n",
    "        Y1_sampled = Y1[indices.flatten()]\n",
    "\n",
    "        # Reshape the sampled X1 and Y1 for the final DataFrame\n",
    "        X1_sampled = X1_sampled.reshape(-1, X.shape[1])\n",
    "        Y1_sampled = Y1_sampled.reshape(-1, 1)\n",
    "\n",
    "        # Combine the matrices (X0 with X1, Y0 with Y1)\n",
    "        X_combined = np.concatenate([X0_expanded, X1_sampled], axis=1)  # Concatenate feature matrices\n",
    "        Y_combined = np.column_stack((Y0_expanded, Y1_sampled))  # Concatenate outcome matrices\n",
    "\n",
    "        # Calculate outcome differences\n",
    "        nu = Y_combined[:,1] - Y_combined[:,0]\n",
    "\n",
    "        return X_combined, nu\n",
    "\n",
    "    def fit(self, X, treatment, y, p, num_samples=3):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (optional): number of treated samples to pair with each control.\n",
    "        \"\"\"\n",
    "        \n",
    "        X, y = convert_pd_to_np(X, y)\n",
    "        X_pair, nu = self.pair_data(X, treatment, y, num_samples)\n",
    "\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_nu.fit(X_pair, nu)\n",
    "\n",
    "        # for group in self.t_groups:\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     X_filt = X[mask]\n",
    "        #     y_filt = y[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "\n",
    "        #     # Calculate variances and treatment effects\n",
    "        #     var_c = (\n",
    "        #         y_filt[w == 0] - self.model_mu.predict(X_filt[w == 0])\n",
    "        #     ).var()\n",
    "        #     self.vars_c[group] = var_c\n",
    "        #     var_t = (\n",
    "        #         y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1])\n",
    "        #     ).var()\n",
    "        #     self.vars_t[group] = var_t\n",
    "\n",
    "        #     # Train treatment models\n",
    "        #     d_c = (self.model_mu.predict(X_filt[w == 0]) - y_filt[w == 0])\n",
    "        #     d_t = (y_filt[w == 1] - self.model_mu.predict(X_filt[w == 1]))\n",
    "        #     self.models_tau_c[group].fit(X_filt[w == 0], d_c)\n",
    "        #     self.models_tau_t[group].fit(X_filt[w == 1], d_t)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        X = np.concatenate([X, X], axis=1)\n",
    "\n",
    "        # if p is None:\n",
    "        #     logger.info(\"Generating propensity score\")\n",
    "        #     p = dict()\n",
    "        #     for group in self.t_groups:\n",
    "        #         p_model = self.propensity_model[group]\n",
    "        #         p[group] = p_model.predict(X)\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "        dhat_cs = {}\n",
    "        dhat_ts = {}\n",
    "\n",
    "        _te = (self.model_nu.predict(X)).reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        te[:, 0] = np.ravel(_te)\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     model_tau_c = self.models_tau_c[group]\n",
    "        #     model_tau_t = self.models_tau_t[group]\n",
    "        #     dhat_cs[group] = model_tau_c.predict(X)\n",
    "        #     dhat_ts[group] = model_tau_t.predict(X)\n",
    "\n",
    "        #     _te = (dhat_cs[group] + dhat_ts[group]).reshape(\n",
    "        #         -1, 1\n",
    "        #     )\n",
    "        #     te[:, i] = np.ravel(_te)\n",
    "\n",
    "        #     if (y is not None) and (treatment is not None) and verbose:\n",
    "        #         mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #         treatment_filt = treatment[mask]\n",
    "        #         X_filt = X[mask]\n",
    "        #         y_filt = y[mask]\n",
    "        #         w = (treatment_filt == group).astype(int)\n",
    "\n",
    "        #         yhat = np.zeros_like(y, dtype=float)\n",
    "        #         yhat = self.model_mu.predict(X)\n",
    "\n",
    "        #         logger.info(\"Error metrics for group {}\".format(group))\n",
    "        #         regression_metrics(y, yhat, w)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, dhat_cs, dhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=3,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p, num_samples)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, p=p, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=self.propensity, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, dhat_cs, dhat_ts = self.predict(\n",
    "                    X, treatment, y, p=p, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, dhat_cs, dhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        ate[0] = te[:, 0].mean()\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     _ate = te[:, i].mean()\n",
    "\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "        #     prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "        #     dhat_c = dhat_cs[group][mask]\n",
    "        #     dhat_t = dhat_ts[group][mask]\n",
    "        #     p_filt = p[group][mask]\n",
    "\n",
    "        #     # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "        #     # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "        #     se = np.sqrt(\n",
    "        #         (\n",
    "        #             self.vars_t[group] / prob_treatment\n",
    "        #             + self.vars_c[group] / (1 - prob_treatment)\n",
    "        #             + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "        #         )\n",
    "        #         / w.shape[0]\n",
    "        #     )\n",
    "\n",
    "        #     _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "        #     _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "        #     ate[i] = _ate\n",
    "        #     ate_lb[i] = _ate_lb\n",
    "        #     ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "class BasePRegressor(BasePLearner):\n",
    "    \"\"\"\n",
    "    A parent class for P-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a P-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRPLearner(BaseLearner):\n",
    "    \"\"\"A parent class for RP-learner regressor classes.\n",
    "\n",
    "    A RP-learner estimates treatment effects with two machine learning models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        pair_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a RP-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            pair_learner (optional): a model to estimate pair outcome differences\n",
    "            treatment_effect_learner (optional): a model to estimate treatment effects in the treatment group\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (outcome_learner is not None)\n",
    "            and (pair_learner is not None)\n",
    "        )\n",
    "\n",
    "        if outcome_learner is None:\n",
    "            self.model_mu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu = outcome_learner\n",
    "\n",
    "        if pair_learner is None:\n",
    "            self.model_nu = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_nu = pair_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "        self.propensity_model = None\n",
    "        self.model_p = LogisticRegression()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(outcome_learner={},\\n\"\n",
    "            \"\\tpair_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu.__repr__(),\n",
    "                self.model_nu.__repr__()\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def pair_data(self, X, treatment, y, num_samples):\n",
    "\n",
    "        # Split T=0 and T=1\n",
    "        X0 = X[treatment == 0]\n",
    "        Y0 = y[treatment == 0]\n",
    "        X1 = X[treatment == 1]\n",
    "        Y1 = y[treatment == 1]\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        X1_scaled = scaler.fit_transform(X1)\n",
    "        X0_scaled = scaler.fit_transform(X0)\n",
    "\n",
    "        # Build KD-tree from Y\n",
    "        tree = cKDTree(X1_scaled)\n",
    "\n",
    "        # Query for k nearest neighbours\n",
    "        distances, indices = tree.query(X0_scaled, k=num_samples)\n",
    "        # Randomly sample rows from X_treated for each row in X_control\n",
    "        random_indices = np.random.randint(0, len(X1), (len(X0), num_samples))\n",
    "\n",
    "        # Expand T=0 data to match shape\n",
    "        X0_expanded = np.repeat(X0, num_samples, axis=0)  # Repeat X0 for 5 times\n",
    "        Y0_expanded = np.repeat(Y0, num_samples, axis=0)  # Repeat Y0 for 5 times\n",
    "\n",
    "        # Get corresponding T=1 data (using the random indices)\n",
    "        X1_sampled = X1[indices.flatten()]\n",
    "        Y1_sampled = Y1[indices.flatten()]\n",
    "\n",
    "        # Reshape the sampled X1 and Y1 for the final DataFrame\n",
    "        X1_sampled = X1_sampled.reshape(-1, X.shape[1])\n",
    "        Y1_sampled = Y1_sampled.reshape(-1, 1)\n",
    "\n",
    "        # Combine the matrices (X0 with X1, Y0 with Y1)\n",
    "        X_combined = np.concatenate([X0_expanded, X1_sampled], axis=1)  # Concatenate feature matrices\n",
    "        Y_combined = np.column_stack((Y0_expanded, Y1_sampled))  # Concatenate outcome matrices\n",
    "\n",
    "        # Calculate outcome differences\n",
    "        nu = Y_combined[:,1] - self.model_mu.predict(X1_sampled) + self.model_mu.predict(X0_expanded) - Y_combined[:,0]\n",
    "\n",
    "        return X_combined, nu\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None, num_samples=3):\n",
    "        \"\"\"Fit the inference model.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int, optional): number of pairs for each control observation\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        # if p is None:\n",
    "        #     self._set_propensity_models(X=X, treatment=treatment, y=y)\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        self.vars_c = {}\n",
    "        self.vars_t = {}\n",
    "\n",
    "        # Train outcome model\n",
    "        self.model_mu.fit(X, y)\n",
    "\n",
    "        # Train pair model\n",
    "        X_paired, nu = self.pair_data(X, treatment, y, num_samples)\n",
    "        self.model_nu.fit(X_paired, nu)\n",
    "\n",
    "    def predict(\n",
    "        self, X, treatment=None, y=None, p=None, return_components=False, verbose=True\n",
    "    ):\n",
    "        \"\"\"Predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series, optional): a treatment vector\n",
    "            y (np.array or pd.Series, optional): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_components (bool, optional): whether to return differences for treatment and control seperately\n",
    "            verbose (bool, optional): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects.\n",
    "        \"\"\"\n",
    "        X= convert_pd_to_np(X)\n",
    "        X_paired = np.concatenate([X, X], axis=1)\n",
    "\n",
    "        # if p is None:\n",
    "        #     logger.info(\"Generating propensity score\")\n",
    "        #     p = dict()\n",
    "        #     for group in self.t_groups:\n",
    "        #         p_model = self.propensity_model[group]\n",
    "        #         p[group] = p_model.predict(X)\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = np.zeros((X.shape[0], self.t_groups.shape[0]))\n",
    "\n",
    "        fhat = self.model_mu.predict(X)\n",
    "        tauhat = self.model_nu.predict(X_paired)\n",
    "\n",
    "        _te = (tauhat).reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        te[:, 0] = np.ravel(_te)\n",
    "\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, fhat\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=3,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int): number of pairs for each control observation\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p, num_samples)\n",
    "\n",
    "        # if p is None:\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        te = self.predict(\n",
    "            X, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.model_nu = deepcopy(model_nu_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        num_samples=3,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            num_samples (int): number of pairs for each control observation\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            if p is None:\n",
    "                # when p is null, use pretrain propensity score\n",
    "                if not self.propensity:\n",
    "                    raise ValueError(\"no propensity score, please call fit() first\")\n",
    "                te, fhat = self.predict(\n",
    "                    X, treatment, y, return_components=True\n",
    "                )\n",
    "            else:\n",
    "                p = self._format_p(p, self.t_groups)\n",
    "                te, fhat = self.predict(\n",
    "                    X, treatment, y, return_components=True\n",
    "                )\n",
    "        else:\n",
    "            te, fhat = self.fit_predict(\n",
    "                X, treatment, y, num_samples, return_components=True\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        # if p is None:\n",
    "        #     p = self.propensity\n",
    "        # else:\n",
    "        #     p = self._format_p(p, self.t_groups)\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        ate[0] = te[:, 0].mean()\n",
    "\n",
    "        # for i, group in enumerate(self.t_groups):\n",
    "        #     _ate = te[:, i].mean()\n",
    "\n",
    "        #     mask = (treatment == group) | (treatment == self.control_name)\n",
    "        #     treatment_filt = treatment[mask]\n",
    "        #     w = (treatment_filt == group).astype(int)\n",
    "        #     prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "        #     dhat_c = dhat_cs[group][mask]\n",
    "        #     dhat_t = dhat_ts[group][mask]\n",
    "        #     p_filt = p[group][mask]\n",
    "\n",
    "        #     # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "        #     # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "        #     se = np.sqrt(\n",
    "        #         (\n",
    "        #             self.vars_t[group] / prob_treatment\n",
    "        #             + self.vars_c[group] / (1 - prob_treatment)\n",
    "        #             + (p_filt * dhat_c + (1 - p_filt) * dhat_t).var()\n",
    "        #         )\n",
    "        #         / w.shape[0]\n",
    "        #     )\n",
    "\n",
    "        #     _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "        #     _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "        #     ate[i] = _ate\n",
    "        #     ate_lb[i] = _ate_lb\n",
    "        #     ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            model_mu_global = deepcopy(self.model_mu)\n",
    "            model_nu_global = deepcopy(self.model_nu)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.model_mu = deepcopy(model_mu_global)\n",
    "            self.models_nu = deepcopy(model_nu_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "class BaseRPRegressor(BaseRPLearner):\n",
    "    \"\"\"\n",
    "    A parent class for RP-learner regressor classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        outcome_learner=None,\n",
    "        pair_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"Initialize a RP-learner regressor.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): a model to estimate outcomes and treatment effects in both the control and treatment\n",
    "                groups\n",
    "            outcome_learner (optional): a model to estimate outcomes\n",
    "            pair_learner (optional): a model to estimate paired treatment effects\n",
    "            ate_alpha (float, optional): the confidence level alpha of the ATE estimate\n",
    "            control_name (str or int, optional): name of control group\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            outcome_learner=outcome_learner,\n",
    "            pair_learner=pair_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePDRLearner(BaseLearner):\n",
    "    \"\"\"\n",
    "    A parent class for PDR-learner regressor classes.\n",
    "    PDR-learner estimates treatment effects with machine learning models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        control_outcome_learner=None,\n",
    "        treatment_outcome_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a PDR-learner.\n",
    "\n",
    "        Args:\n",
    "            learner (optional): model used for all tasks if specific learners are not provided.\n",
    "            control_outcome_learner (optional): model for control outcomes.\n",
    "            treatment_outcome_learner (optional): model for treated outcomes.\n",
    "            treatment_effect_learner (optional): model for treatment effects.\n",
    "            ate_alpha (float, optional): significance level for ATE CI.\n",
    "            control_name (str or int, optional): label for control group.\n",
    "        \"\"\"\n",
    "        assert (learner is not None) or (\n",
    "            (control_outcome_learner is not None)\n",
    "            and (treatment_outcome_learner is not None)\n",
    "            and (treatment_effect_learner is not None)\n",
    "        )\n",
    "\n",
    "        if control_outcome_learner is None:\n",
    "            self.model_mu_c = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu_c = control_outcome_learner\n",
    "\n",
    "        if treatment_outcome_learner is None:\n",
    "            self.model_mu_t = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_mu_t = treatment_outcome_learner\n",
    "\n",
    "        if treatment_effect_learner is None:\n",
    "            self.model_tau = deepcopy(learner)\n",
    "        else:\n",
    "            self.model_tau = treatment_effect_learner\n",
    "\n",
    "        self.ate_alpha = ate_alpha\n",
    "        self.control_name = control_name\n",
    "\n",
    "        self.propensity = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            \"{}(control_outcome_learner={},\\n\"\n",
    "            \"\\ttreatment_outcome_learner={},\\n\"\n",
    "            \"\\ttreatment_effect_learner={})\".format(\n",
    "                self.__class__.__name__,\n",
    "                self.model_mu_c.__repr__(),\n",
    "                self.model_mu_t.__repr__(),\n",
    "                self.model_tau.__repr__(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def fit(self, X, treatment, y, p=None, seed=None):\n",
    "        \"\"\"\n",
    "        Fit the PDR-learner with the doubly robust pairwise estimator.\n",
    "\n",
    "        Args:\n",
    "            X (np.array or pd.DataFrame): feature matrix.\n",
    "            treatment (np.array or pd.Series): treatment vector.\n",
    "            y (np.array or pd.Series): outcome vector.\n",
    "            p (optional): propensity scores; if None, they are estimated.\n",
    "            seed (int): random seed.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        check_treatment_vector(treatment, self.control_name)\n",
    "        self.t_groups = np.unique(treatment[treatment != self.control_name])\n",
    "        self.t_groups.sort()\n",
    "        self._classes = {group: i for i, group in enumerate(self.t_groups)}\n",
    "\n",
    "        # Use 3-fold cross-fitting\n",
    "        cv = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "        split_indices = [index for _, index in cv.split(y)]\n",
    "\n",
    "        self.models_mu_c = [deepcopy(self.model_mu_c) for _ in range(3)]\n",
    "        self.models_mu_t = {\n",
    "            group: [deepcopy(self.model_mu_t) for _ in range(3)]\n",
    "            for group in self.t_groups\n",
    "        }\n",
    "        self.models_tau = {\n",
    "            group: [deepcopy(self.model_tau) for _ in range(3)]\n",
    "            for group in self.t_groups\n",
    "        }\n",
    "\n",
    "        # Initialize propensity score container if not provided\n",
    "        if p is None:\n",
    "            self.propensity = {group: np.zeros(y.shape[0]) for group in self.t_groups}\n",
    "\n",
    "        # Cross-fit\n",
    "        for ifold in range(3):\n",
    "            treatment_idx = split_indices[ifold]\n",
    "            outcome_idx = split_indices[(ifold + 1) % 3]\n",
    "            tau_idx = split_indices[(ifold + 2) % 3]\n",
    "\n",
    "            treatment_treat = treatment[treatment_idx]\n",
    "            treatment_out = treatment[outcome_idx]\n",
    "            treatment_tau = treatment[tau_idx]\n",
    "\n",
    "            y_out, y_tau = y[outcome_idx], y[tau_idx]\n",
    "            X_treat, X_out, X_tau = X[treatment_idx], X[outcome_idx], X[tau_idx]\n",
    "\n",
    "            # Propensity score estimation if not provided\n",
    "            if p is None:\n",
    "                logger.info(\"Estimating propensity scores\")\n",
    "                cur_p = dict()\n",
    "                for group in self.t_groups:\n",
    "                    mask = (treatment_treat == group) | (treatment_treat == self.control_name)\n",
    "                    X_filt = X_treat[mask]\n",
    "                    treatment_filt = treatment_treat[mask]\n",
    "                    w_filt = (treatment_filt == group).astype(int)\n",
    "                    # Compute propensity scores for group 'group'\n",
    "                    cur_p[group], _ = compute_propensity_score(\n",
    "                        X=X_filt, treatment=w_filt, X_pred=X_tau, treatment_pred=(treatment_tau == group).astype(int)\n",
    "                    )\n",
    "                    self.propensity[group][tau_idx] = cur_p[group]\n",
    "            else:\n",
    "                cur_p = dict()\n",
    "                if isinstance(p, (np.ndarray, pd.Series)):\n",
    "                    cur_p = {self.t_groups[0]: convert_pd_to_np(p[tau_idx])}\n",
    "                else:\n",
    "                    cur_p = {g: p[g][tau_idx] for g in self.t_groups}\n",
    "                check_p_conditions(cur_p, self.t_groups)\n",
    "\n",
    "            # Outcome regression: fit control and treatment models on outcome fold\n",
    "            logger.info(\"Fitting outcome regressions\")\n",
    "            # Fit control outcome model on control units\n",
    "            self.models_mu_c[ifold].fit(\n",
    "                X_out[treatment_out == self.control_name],\n",
    "                y_out[treatment_out == self.control_name],\n",
    "            )\n",
    "            for group in self.t_groups:\n",
    "                # Fit treated outcome model for each group\n",
    "                self.models_mu_t[group][ifold].fit(\n",
    "                    X_out[treatment_out == group],\n",
    "                    y_out[treatment_out == group],\n",
    "                )\n",
    "\n",
    "            # Fit pseudo outcomes for treatment effect using doubly robust pairwise estimator\n",
    "            logger.info(\"Fitting doubly robust pairwise pseudo outcomes\")\n",
    "            for group in self.t_groups:\n",
    "                # Filter tau-fold: keep observations from control or group 'group'\n",
    "                mask = (treatment_tau == group) | (treatment_tau == self.control_name)\n",
    "                X_filt = X_tau[mask]\n",
    "                y_filt = y_tau[mask]\n",
    "                p_filt = cur_p[group][mask]\n",
    "                # Predict outcomes using outcome models\n",
    "                mu_c_all = self.models_mu_c[ifold].predict(X_filt)\n",
    "                mu_t_all = self.models_mu_t[group][ifold].predict(X_filt)\n",
    "\n",
    "                # Separate into control and treated subsets for pairing\n",
    "                mask_control = (treatment_tau[mask] == self.control_name)\n",
    "                mask_treated = (treatment_tau[mask] == group)\n",
    "                if np.sum(mask_control) == 0 or np.sum(mask_treated) == 0:\n",
    "                    logger.warning(\"Not enough data for pairing in group {}\".format(group))\n",
    "                    continue\n",
    "\n",
    "                X_control = X_filt[mask_control]\n",
    "                y_control = y_filt[mask_control]\n",
    "                p_control = p_filt[mask_control]\n",
    "                mu_control = mu_c_all[mask_control]\n",
    "\n",
    "                X_treated = X_filt[mask_treated]\n",
    "                y_treated = y_filt[mask_treated]\n",
    "                p_treated = p_filt[mask_treated]\n",
    "                mu_treated = mu_t_all[mask_treated]\n",
    "\n",
    "                scaler = MinMaxScaler()\n",
    "\n",
    "                X_control_scaled = scaler.fit_transform(X_control)\n",
    "                X_treated_scaled = scaler.fit_transform(X_treated)\n",
    "\n",
    "                # Build KDTree on the control group\n",
    "                tree = cKDTree(X_control_scaled)\n",
    "\n",
    "                k = 10\n",
    "\n",
    "                # Find the nearest control neighbour for each treated sample\n",
    "                distances, idx_control = tree.query(X_treated_scaled, k=k)  # k=5\n",
    "\n",
    "                # Expand treated indices to match the number of pairs (each treated unit repeats k times)\n",
    "                idx_treated = np.repeat(np.arange(len(X_treated)), k)\n",
    "                idx_control = idx_control.flatten()  # Flatten to align with repeated treated indices\n",
    "\n",
    "\n",
    "                # Compute the doubly robust pairwise pseudo outcome for each pair:\n",
    "                #   dr_pair = [mu_treated - mu_control] +\n",
    "                #             [ (y_treated - mu_treated) / p_treated - (y_control - mu_control) / (1-p_control) ]\n",
    "                dr_pairs = (\n",
    "                    (mu_treated[idx_treated] - mu_control[idx_control])\n",
    "                    + ((y_treated[idx_treated] - mu_treated[idx_treated]) / p_treated[idx_treated]\n",
    "                       - (y_control[idx_control] - mu_control[idx_control]) / (1 - p_control[idx_control]))\n",
    "                )\n",
    "                # Combine the paired features. Here we simply concatenate the control and treated features.\n",
    "                X_pairs = np.hstack([X_control[idx_control], X_treated[idx_treated]])\n",
    "                # Fit the treatment effect learner on the paired data and doubly robust pseudo outcomes.\n",
    "                self.models_tau[group][ifold].fit(X_pairs, dr_pairs)\n",
    "\n",
    "    def predict(self, X, treatment=None, y=None, p=None, return_components=False, verbose=True):\n",
    "        \"\"\"\n",
    "        Predict treatment effects.\n",
    "        Args:\n",
    "            X (np.array or pd.DataFrame): feature matrix.\n",
    "            treatment (optional): treatment vector.\n",
    "            y (optional): outcome vector.\n",
    "            return_components (bool): if True, return predicted outcomes for control and treated separately.\n",
    "            verbose (bool): whether to output logs.\n",
    "        Returns:\n",
    "            np.array: predicted treatment effects.\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        te = np.zeros((X.shape[0], len(self.t_groups)))\n",
    "        yhat_cs = {}\n",
    "        yhat_ts = {}\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            # Average the treatment effect predictions from the cross-fit models\n",
    "            models_tau = self.models_tau[group]\n",
    "            _te = np.r_[[model.predict(np.hstack([X, X])) for model in models_tau]].mean(axis=0)\n",
    "            te[:, i] = np.ravel(_te)\n",
    "            yhat_cs[group] = np.r_[\n",
    "                [model.predict(X) for model in self.models_mu_c]\n",
    "            ].mean(axis=0)\n",
    "            yhat_ts[group] = np.r_[\n",
    "                [model.predict(X) for model in self.models_mu_t[group]]\n",
    "            ].mean(axis=0)\n",
    "\n",
    "            if (y is not None) and (treatment is not None) and verbose:\n",
    "                mask = (treatment == group) | (treatment == self.control_name)\n",
    "                treatment_filt = treatment[mask]\n",
    "                y_filt = y[mask]\n",
    "                w = (treatment_filt == group).astype(int)\n",
    "                yhat = np.zeros_like(y_filt, dtype=float)\n",
    "                yhat[w == 0] = yhat_cs[group][mask][w == 0]\n",
    "                yhat[w == 1] = yhat_ts[group][mask][w == 1]\n",
    "                logger.info(\"Error metrics for group {}\".format(group))\n",
    "                regression_metrics(y_filt, yhat, w)\n",
    "        if not return_components:\n",
    "            return te\n",
    "        else:\n",
    "            return te, yhat_cs, yhat_ts\n",
    "\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        return_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        return_components=False,\n",
    "        verbose=True,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"Fit the treatment effect and outcome models of the R learner and predict treatment effects.\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            return_ci (bool): whether to return confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            return_components (bool, optional): whether to return outcome for treatment and control seperately\n",
    "            verbose (str): whether to output progress logs\n",
    "            seed (int): random seed for cross-fitting\n",
    "        Returns:\n",
    "            (numpy.ndarray): Predictions of treatment effects. Output dim: [n_samples, n_treatment]\n",
    "                If return_ci, returns CATE [n_samples, n_treatment], LB [n_samples, n_treatment],\n",
    "                UB [n_samples, n_treatment]\n",
    "        \"\"\"\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        self.fit(X, treatment, y, p, seed)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "\n",
    "        check_p_conditions(p, self.t_groups)\n",
    "        if isinstance(p, (np.ndarray, pd.Series)):\n",
    "            treatment_name = self.t_groups[0]\n",
    "            p = {treatment_name: convert_pd_to_np(p)}\n",
    "        elif isinstance(p, dict):\n",
    "            p = {\n",
    "                treatment_name: convert_pd_to_np(_p) for treatment_name, _p in p.items()\n",
    "            }\n",
    "\n",
    "        te = self.predict(\n",
    "            X, treatment=treatment, y=y, return_components=return_components\n",
    "        )\n",
    "\n",
    "        if not return_ci:\n",
    "            return te\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            models_mu_c_global = deepcopy(self.models_mu_c)\n",
    "            models_mu_t_global = deepcopy(self.models_mu_t)\n",
    "            models_tau_global = deepcopy(self.models_tau)\n",
    "            te_bootstraps = np.zeros(\n",
    "                shape=(X.shape[0], self.t_groups.shape[0], n_bootstraps)\n",
    "            )\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals\")\n",
    "            for i in tqdm(range(n_bootstraps)):\n",
    "                te_b = self.bootstrap(X, treatment, y, p, size=bootstrap_size)\n",
    "                te_bootstraps[:, :, i] = te_b\n",
    "\n",
    "            te_lower = np.percentile(te_bootstraps, (self.ate_alpha / 2) * 100, axis=2)\n",
    "            te_upper = np.percentile(\n",
    "                te_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=2\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.models_mu_c = deepcopy(models_mu_c_global)\n",
    "            self.models_mu_t = deepcopy(models_mu_t_global)\n",
    "            self.models_tau = deepcopy(models_tau_global)\n",
    "\n",
    "            return (te, te_lower, te_upper)\n",
    "\n",
    "    def estimate_ate(\n",
    "        self,\n",
    "        X,\n",
    "        treatment,\n",
    "        y,\n",
    "        p=None,\n",
    "        bootstrap_ci=False,\n",
    "        n_bootstraps=1000,\n",
    "        bootstrap_size=10000,\n",
    "        seed=None,\n",
    "        pretrain=False,\n",
    "    ):\n",
    "        \"\"\"Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (np.matrix or np.array or pd.Dataframe): a feature matrix\n",
    "            treatment (np.array or pd.Series): a treatment vector\n",
    "            y (np.array or pd.Series): an outcome vector\n",
    "            p (np.ndarray or pd.Series or dict, optional): an array of propensity scores of float (0,1) in the\n",
    "                single-treatment case; or, a dictionary of treatment groups that map to propensity vectors of\n",
    "                float (0,1); if None will run ElasticNetPropensityModel() to generate the propensity scores.\n",
    "            bootstrap_ci (bool): whether run bootstrap for confidence intervals\n",
    "            n_bootstraps (int): number of bootstrap iterations\n",
    "            bootstrap_size (int): number of samples per bootstrap\n",
    "            seed (int): random seed for cross-fitting\n",
    "            pretrain (bool): whether a model has been fit, default False.\n",
    "        Returns:\n",
    "            The mean and confidence interval (LB, UB) of the ATE estimate.\n",
    "        \"\"\"\n",
    "        if pretrain:\n",
    "            te, yhat_cs, yhat_ts = self.predict(\n",
    "                X, treatment, y, p, return_components=True\n",
    "            )\n",
    "        else:\n",
    "            te, yhat_cs, yhat_ts = self.fit_predict(\n",
    "                X, treatment, y, p, return_components=True, seed=seed\n",
    "            )\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "\n",
    "        if p is None:\n",
    "            p = self.propensity\n",
    "        else:\n",
    "            check_p_conditions(p, self.t_groups)\n",
    "        if isinstance(p, (np.ndarray, pd.Series)):\n",
    "            treatment_name = self.t_groups[0]\n",
    "            p = {treatment_name: convert_pd_to_np(p)}\n",
    "        elif isinstance(p, dict):\n",
    "            p = {\n",
    "                treatment_name: convert_pd_to_np(_p) for treatment_name, _p in p.items()\n",
    "            }\n",
    "\n",
    "        ate = np.zeros(self.t_groups.shape[0])\n",
    "        ate_lb = np.zeros(self.t_groups.shape[0])\n",
    "        ate_ub = np.zeros(self.t_groups.shape[0])\n",
    "\n",
    "        for i, group in enumerate(self.t_groups):\n",
    "            _ate = te[:, i].mean()\n",
    "\n",
    "            mask = (treatment == group) | (treatment == self.control_name)\n",
    "            treatment_filt = treatment[mask]\n",
    "            w = (treatment_filt == group).astype(int)\n",
    "            prob_treatment = float(sum(w)) / w.shape[0]\n",
    "\n",
    "            yhat_c = yhat_cs[group][mask]\n",
    "            yhat_t = yhat_ts[group][mask]\n",
    "            y_filt = y[mask]\n",
    "\n",
    "            # SE formula is based on the lower bound formula (7) from Imbens, Guido W., and Jeffrey M. Wooldridge. 2009.\n",
    "            # \"Recent Developments in the Econometrics of Program Evaluation.\" Journal of Economic Literature\n",
    "            se = np.sqrt(\n",
    "                (\n",
    "                    (y_filt[w == 0] - yhat_c[w == 0]).var() / (1 - prob_treatment)\n",
    "                    + (y_filt[w == 1] - yhat_t[w == 1]).var() / prob_treatment\n",
    "                    + (yhat_t - yhat_c).var()\n",
    "                )\n",
    "                / y_filt.shape[0]\n",
    "            )\n",
    "\n",
    "            _ate_lb = _ate - se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "            _ate_ub = _ate + se * norm.ppf(1 - self.ate_alpha / 2)\n",
    "\n",
    "            ate[i] = _ate\n",
    "            ate_lb[i] = _ate_lb\n",
    "            ate_ub[i] = _ate_ub\n",
    "\n",
    "        if not bootstrap_ci:\n",
    "            return ate, ate_lb, ate_ub\n",
    "        else:\n",
    "            t_groups_global = self.t_groups\n",
    "            _classes_global = self._classes\n",
    "            models_mu_c_global = deepcopy(self.models_mu_c)\n",
    "            models_mu_t_global = deepcopy(self.models_mu_t)\n",
    "            models_tau_global = deepcopy(self.models_tau)\n",
    "\n",
    "            logger.info(\"Bootstrap Confidence Intervals for ATE\")\n",
    "            ate_bootstraps = np.zeros(shape=(self.t_groups.shape[0], n_bootstraps))\n",
    "\n",
    "            for n in tqdm(range(n_bootstraps)):\n",
    "                cate_b = self.bootstrap(\n",
    "                    X, treatment, y, p, size=bootstrap_size, seed=seed\n",
    "                )\n",
    "                ate_bootstraps[:, n] = cate_b.mean()\n",
    "\n",
    "            ate_lower = np.percentile(\n",
    "                ate_bootstraps, (self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "            ate_upper = np.percentile(\n",
    "                ate_bootstraps, (1 - self.ate_alpha / 2) * 100, axis=1\n",
    "            )\n",
    "\n",
    "            # set member variables back to global (currently last bootstrapped outcome)\n",
    "            self.t_groups = t_groups_global\n",
    "            self._classes = _classes_global\n",
    "            self.models_mu_c = deepcopy(models_mu_c_global)\n",
    "            self.models_mu_t = deepcopy(models_mu_t_global)\n",
    "            self.models_tau = deepcopy(models_tau_global)\n",
    "            return ate, ate_lower, ate_upper\n",
    "\n",
    "class BasePDRRegressor(BasePDRLearner):\n",
    "    \"\"\"\n",
    "    A parent class for PDR-learner regressor classes.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        learner=None,\n",
    "        control_outcome_learner=None,\n",
    "        treatment_outcome_learner=None,\n",
    "        treatment_effect_learner=None,\n",
    "        ate_alpha=0.05,\n",
    "        control_name=0,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            learner=learner,\n",
    "            control_outcome_learner=control_outcome_learner,\n",
    "            treatment_outcome_learner=treatment_outcome_learner,\n",
    "            treatment_effect_learner=treatment_effect_learner,\n",
    "            ate_alpha=ate_alpha,\n",
    "            control_name=control_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Create the neural network\n",
    "def _build_mlp(input_dim, hidden_dims, output_dim=1, activation=nn.ReLU):\n",
    "    \"\"\"Builds a simple MLP.\"\"\"\n",
    "    layers = []\n",
    "    last_dim = input_dim\n",
    "    for hidden_dim in hidden_dims:\n",
    "        layers.append(nn.Linear(last_dim, hidden_dim))\n",
    "        layers.append(activation())\n",
    "        last_dim = hidden_dim\n",
    "    layers.append(nn.Linear(last_dim, output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Class for pairs\n",
    "class PairDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset to serve pairs of indices.\"\"\"\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs # List of tuples (idx_i, idx_j)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return the pair of original indices\n",
    "        return self.pairs[idx]\n",
    "\n",
    "# PairNet implementation using PyTorch\n",
    "class PairNetTorch:\n",
    "    \"\"\"\n",
    "    PairNet implementation using PyTorch.\n",
    "\n",
    "    Uses a T-Learner architecture (separate networks for control and treatment)\n",
    "    trained jointly with the loss: ((y_i - y_j) - (mu(xi, ti) - mu(xj, tj)))^2\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dims: list = [64, 32],\n",
    "                 activation = nn.ReLU,\n",
    "                 learning_rate: float = 1e-3,\n",
    "                 weight_decay: float = 1e-4,\n",
    "                 epochs: int = 100,\n",
    "                 batch_size: int = 200,\n",
    "                 distance_threshold: float = np.inf,\n",
    "                 num_neighbours: int = 3, \n",
    "                 val_size: float = 0.0, # proportion used for validation, in [0,1]\n",
    "                 patience: int = 10,     # Early stopping patience\n",
    "                 verbose: int = 10,     # Print loss every `verbose` epochs\n",
    "                 random_state: int | None = None,\n",
    "                 device: str | None = None):\n",
    "        \"\"\"\n",
    "        Initialize the PairNet learner.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Number of features in X.\n",
    "            hidden_dims (list, optional): List of hidden layer sizes for mu_c and mu_t.\n",
    "                                          Defaults to [64, 32].\n",
    "            activation (torch.nn.Module, optional): Activation function. Defaults to nn.ReLU.\n",
    "            learning_rate (float, optional): Optimizer learning rate. Defaults to 1e-3.\n",
    "            weight_decay (float, optional): L2 regularization strength. Defaults to 1e-4.\n",
    "            epochs (int, optional): Number of training epochs. Defaults to 100.\n",
    "            batch_size (int, optional): Batch size for training pairs. Defaults to 200.\n",
    "            distance_threshold (float, optional): Max distance for pairing. Defaults to no max distance.\n",
    "            num_neighbours (int, optional): neighbours per sample in pairing. Defaults to 3.\n",
    "            val_size (float, optional): Fraction of data to use for validation/early stopping.\n",
    "                                        If 0, no validation/early stopping is used. Defaults to 0.\n",
    "            patience (int, optional): Early stopping patience (epochs without improvement). Defaults to 10.\n",
    "            verbose (int, optional): Print loss frequency (epochs). Defaults to 10.\n",
    "            random_state (int | None, optional): Random seed. Defaults to None.\n",
    "            device (str | None, optional): Device ('cpu', 'cuda'). Autodetects if None.\n",
    "        \"\"\"\n",
    "        \n",
    "        if random_state is not None:\n",
    "            torch.manual_seed(random_state)\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.activation = activation\n",
    "        self.lr = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.distance_threshold = distance_threshold\n",
    "        self.num_neighbours = num_neighbours\n",
    "        self.val_size = val_size\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.default_rng(random_state)\n",
    "\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Initialize models, optimizers, scaler\n",
    "        self.mu_c = _build_mlp(input_dim, hidden_dims, 1, activation).to(self.device)\n",
    "        self.mu_t = _build_mlp(input_dim, hidden_dims, 1, activation).to(self.device)\n",
    "\n",
    "        # Combine parameters for a single optimizer pass\n",
    "        all_params = list(self.mu_c.parameters()) + list(self.mu_t.parameters())\n",
    "        self.optimizer = optim.Adam(all_params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        self.scaler = StandardScaler() # Scale features for NN stability\n",
    "\n",
    "        # Placeholders for data and results\n",
    "        self.X_tensor = None\n",
    "        self.t_tensor = None\n",
    "        self.y_tensor = None\n",
    "        self.pairs_train = []\n",
    "        self.pairs_val = []\n",
    "        self.best_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "        self.best_mu_c_state = None\n",
    "        self.best_mu_t_state = None\n",
    "\n",
    "\n",
    "    def _softmax(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Numerically stable softmax function.\"\"\"\n",
    "        if x.size == 0: \n",
    "            return np.array([])\n",
    "        return softmax(x)\n",
    "\n",
    "    def _create_pairs_torch(self, X_np: np.ndarray, t_np: np.ndarray) -> list[tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Creates pairs based on distance in the original feature space.\n",
    "        Returns a list of (idx_i, idx_j) tuples.\n",
    "        \"\"\"\n",
    "        n_samples = X_np.shape[0]\n",
    "        idx = np.arange(n_samples)\n",
    "        idx_c = idx[t_np == 0]\n",
    "        idx_t = idx[t_np == 1]\n",
    "\n",
    "        if len(idx_c) == 0 or len(idx_t) == 0:\n",
    "            print(\"Warning: No samples in control or treatment group for pairing.\")\n",
    "            return []\n",
    "\n",
    "        X_c = X_np[idx_c]\n",
    "        X_t = X_np[idx_t]\n",
    "\n",
    "        all_pairs = [] # List of (original_index_i, original_index_j)\n",
    "\n",
    "        # --- Function to process one direction (e.g., target=Treated, pool=Control) ---\n",
    "        def find_pairs_one_direction(target_indices, target_X, pool_indices, pool_X):\n",
    "            local_pairs = []\n",
    "            if len(target_X) == 0 or len(pool_X) == 0:\n",
    "                return local_pairs\n",
    "\n",
    "            distances = cdist(target_X, pool_X, metric='euclidean')\n",
    "\n",
    "            for i in range(len(target_X)):\n",
    "                dists_i = distances[i, :]\n",
    "                original_target_idx = target_indices[i]\n",
    "\n",
    "                # Softmax Sampling Logic\n",
    "                potential_neighbour_indices = np.where(dists_i <= self.distance_threshold)[0]\n",
    "                if len(potential_neighbour_indices) == 0: \n",
    "                    continue\n",
    "\n",
    "                valid_distances = dists_i[potential_neighbour_indices]\n",
    "                neg_distances = -valid_distances\n",
    "                probs = self._softmax(neg_distances)\n",
    "                if np.any(np.isnan(probs)) or not np.isclose(np.sum(probs), 1.0): \n",
    "                    probs = None\n",
    "\n",
    "                num_to_sample = min(self.num_neighbours, len(potential_neighbour_indices))\n",
    "                if num_to_sample == 0: \n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    sampled_local_indices = self.rng.choice(\n",
    "                        potential_neighbour_indices, size=num_to_sample, replace=True, p=probs\n",
    "                    )\n",
    "                except ValueError as e: # Catch potential issue with p summing slightly off 1.0\n",
    "                    print(f\"Warning: RNG choice error (likely prob sum issue) for sample {original_target_idx}: {e}. Sampling uniformly.\")\n",
    "                    sampled_local_indices = self.rng.choice(\n",
    "                        potential_neighbour_indices, size=num_to_sample, replace=True # Uniform sampling\n",
    "                    )\n",
    "\n",
    "                for local_idx in sampled_local_indices:\n",
    "                    original_pool_idx = pool_indices[local_idx]\n",
    "                    local_pairs.append((original_target_idx, original_pool_idx))\n",
    "\n",
    "            return local_pairs\n",
    "\n",
    "        # Find pairs for treated units (target=Treated, pool=Control)\n",
    "        pairs_tc = find_pairs_one_direction(idx_t, X_t, idx_c, X_c)\n",
    "        all_pairs.extend(pairs_tc)\n",
    "\n",
    "        # Find pairs for control units (target=Control, pool=Treated)\n",
    "        pairs_ct = find_pairs_one_direction(idx_c, X_c, idx_t, X_t)\n",
    "        all_pairs.extend(pairs_ct)\n",
    "\n",
    "        return all_pairs\n",
    "\n",
    "    def _get_predictions_for_pair(self, x_i, t_i, x_j, t_j):\n",
    "        \"\"\"Gets mu(x,t) predictions for both elements of pairs in a batch.\"\"\"\n",
    "        # Predict all i's and j's under both control and treatment scenarios\n",
    "        pred_i_c = self.mu_c(x_i)\n",
    "        pred_i_t = self.mu_t(x_i)\n",
    "        pred_j_c = self.mu_c(x_j)\n",
    "        pred_j_t = self.mu_t(x_j)\n",
    "\n",
    "        # Select the correct prediction based on actual treatment\n",
    "        t_i_mask = t_i.bool() # Convert to boolean mask\n",
    "        t_j_mask = t_j.bool()\n",
    "\n",
    "        pred_i = torch.where(t_i_mask, pred_i_t, pred_i_c)\n",
    "        pred_j = torch.where(t_j_mask, pred_j_t, pred_j_c)\n",
    "\n",
    "        return pred_i, pred_j\n",
    "\n",
    "    def _calculate_loss(self, idx_i_batch, idx_j_batch):\n",
    "        \"\"\"Calculates the pair loss for a batch of pairs.\"\"\"\n",
    "        # Retrieve data for the batch using indices\n",
    "        x_i = self.X_tensor[idx_i_batch]\n",
    "        t_i = self.t_tensor[idx_i_batch]\n",
    "        y_i = self.y_tensor[idx_i_batch]\n",
    "\n",
    "        x_j = self.X_tensor[idx_j_batch]\n",
    "        t_j = self.t_tensor[idx_j_batch]\n",
    "        y_j = self.y_tensor[idx_j_batch]\n",
    "\n",
    "        # Get predictions\n",
    "        pred_i, pred_j = self._get_predictions_for_pair(x_i, t_i, x_j, t_j)\n",
    "\n",
    "        # Calculate the pair loss\n",
    "        true_diff = y_i - y_j\n",
    "        pred_diff = pred_i - pred_j\n",
    "        loss = torch.mean((true_diff - pred_diff) ** 2) \n",
    "        return loss\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, treatment: pd.Series, y: pd.Series, total_time=None):\n",
    "        \"\"\"\n",
    "        Fit the PairNet model using the pair loss.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix.\n",
    "            treatment (pd.Series): Treatment assignment vector (0 or 1).\n",
    "            y (pd.Series): Outcome vector.\n",
    "            total_time (float): Time allowed to run.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        # Preprocessing and Data Setup\n",
    "        X, treatment, y = convert_pd_to_np(X, treatment, y)\n",
    "        X_np = self.scaler.fit_transform(X).astype(np.float32)\n",
    "        t_np = treatment.astype(np.float32).reshape(-1, 1) # Ensure shape [n, 1]\n",
    "        y_np = y.astype(np.float32).reshape(-1, 1)       # Ensure shape [n, 1]\n",
    "\n",
    "        self.X_tensor = torch.tensor(X_np, dtype=torch.float32).to(self.device)\n",
    "        self.t_tensor = torch.tensor(t_np, dtype=torch.float32).to(self.device)\n",
    "        self.y_tensor = torch.tensor(y_np, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Create pairs\n",
    "        all_pairs = self._create_pairs_torch(X_np, t_np.flatten())\n",
    "        if not all_pairs:\n",
    "            raise ValueError(\"No pairs were created. Cannot train the model.\")\n",
    "\n",
    "        # Split pairs for Train/Validation (if val_size > 0)\n",
    "        if self.val_size > 0:\n",
    "            if len(all_pairs) < 2 / self.val_size or len(all_pairs) < 2 / (1 - self.val_size): # Ensure enough pairs for split\n",
    "                 print(\"Warning: Not enough pairs for validation split, disabling validation.\")\n",
    "                 self.val_size = 0\n",
    "                 self.pairs_train = all_pairs\n",
    "                 self.pairs_val = []\n",
    "            else:\n",
    "                 pairs_train_idx, pairs_val_idx = train_test_split(\n",
    "                    np.arange(len(all_pairs)), test_size=self.val_size, random_state=self.random_state\n",
    "                 )\n",
    "                 self.pairs_train = [all_pairs[i] for i in pairs_train_idx]\n",
    "                 self.pairs_val = [all_pairs[i] for i in pairs_val_idx]\n",
    "        else:\n",
    "             self.pairs_train = all_pairs\n",
    "             self.pairs_val = []\n",
    "\n",
    "        train_dataset = PairDataset(self.pairs_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        if self.pairs_val:\n",
    "            val_dataset = PairDataset(self.pairs_val)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "        # Training Loop\n",
    "        self.mu_c.train()\n",
    "        self.mu_t.train()\n",
    "        self.best_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_train_loss = 0.0\n",
    "            for i, (idx_i_batch, idx_j_batch) in enumerate(train_loader):\n",
    "                idx_i_batch = idx_i_batch.long()\n",
    "                idx_j_batch = idx_j_batch.long()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss = self._calculate_loss(idx_i_batch, idx_j_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            avg_epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "\n",
    "            # Validation and Early Stopping\n",
    "            avg_epoch_val_loss = -1\n",
    "            if self.pairs_val:\n",
    "                self.mu_c.eval()\n",
    "                self.mu_t.eval()\n",
    "                epoch_val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for idx_i_batch, idx_j_batch in val_loader:\n",
    "                        idx_i_batch = idx_i_batch.long()\n",
    "                        idx_j_batch = idx_j_batch.long()\n",
    "                        loss = self._calculate_loss(idx_i_batch, idx_j_batch)\n",
    "                        epoch_val_loss += loss.item()\n",
    "                avg_epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "\n",
    "                if avg_epoch_val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = avg_epoch_val_loss\n",
    "                    self.epochs_no_improve = 0\n",
    "                    # Store the best model state\n",
    "                    self.best_mu_c_state = self.mu_c.state_dict()\n",
    "                    self.best_mu_t_state = self.mu_t.state_dict()\n",
    "                else:\n",
    "                    self.epochs_no_improve += 1\n",
    "\n",
    "                # Set back to train mode\n",
    "                self.mu_c.train()\n",
    "                self.mu_t.train()\n",
    "\n",
    "            if self.verbose > 0 and (epoch + 1) % self.verbose == 0:\n",
    "                if self.pairs_val:\n",
    "                    print(f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_epoch_val_loss:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch [{epoch+1}/{self.epochs}], Train Loss: {avg_epoch_train_loss:.4f}\")\n",
    "\n",
    "            if self.val_size > 0 and self.epochs_no_improve >= self.patience:\n",
    "                #print(f\"Early stopping triggered after epoch {epoch+1}. Best Val Loss: {self.best_val_loss:.4f}\")\n",
    "                # Load the best model state before breaking\n",
    "                if self.best_mu_c_state and self.best_mu_t_state:\n",
    "                    self.mu_c.load_state_dict(self.best_mu_c_state)\n",
    "                    self.mu_t.load_state_dict(self.best_mu_t_state)\n",
    "                break\n",
    "            \n",
    "            # Break if used time allowed\n",
    "            elapsed_time = time.perf_counter() - start_time\n",
    "            if elapsed_time > total_time:\n",
    "                break\n",
    "\n",
    "        # If not early stopping or if early stopping happened on last epoch,\n",
    "        # potentially load the best model state if validation was used.\n",
    "        if self.val_size > 0 and self.best_mu_c_state and self.best_mu_t_state:\n",
    "             self.mu_c.load_state_dict(self.best_mu_c_state)\n",
    "             self.mu_t.load_state_dict(self.best_mu_t_state)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: pd.DataFrame, return_components: bool = False) -> np.ndarray | tuple:\n",
    "        \"\"\"\n",
    "        Predict Conditional Average Treatment Effect (CATE).\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix for prediction.\n",
    "            return_components (bool, optional): If True, return tuple of\n",
    "                                                (cate, y_pred_c, y_pred_t).\n",
    "                                                Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray or tuple: Predicted CATE = mu_t(X) - mu_c(X)\n",
    "                                 (or tuple if return_components is True).\n",
    "        \"\"\"\n",
    "        if self.X_tensor is None: # Check if fit has been called\n",
    "            raise RuntimeError(\"Model has not been fitted yet. Call fit() first.\")\n",
    "\n",
    "        self.mu_c.eval()\n",
    "        self.mu_t.eval()\n",
    "\n",
    "        X = convert_pd_to_np(X)\n",
    "        X_np = self.scaler.transform(X).astype(np.float32) # Scale data\n",
    "        X_pred_tensor = torch.tensor(X_np, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_c = self.mu_c(X_pred_tensor)\n",
    "            y_pred_t = self.mu_t(X_pred_tensor)\n",
    "\n",
    "        cate_pred = y_pred_t - y_pred_c\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        cate_pred_np = cate_pred.cpu().numpy()\n",
    "        if return_components:\n",
    "            y_pred_c_np = y_pred_c.cpu().numpy()\n",
    "            y_pred_t_np = y_pred_t.cpu().numpy()\n",
    "            return cate_pred_np, y_pred_c_np, y_pred_t_np\n",
    "        else:\n",
    "            return cate_pred_np\n",
    "\n",
    "    def estimate_ate(self, X: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Estimate the Average Treatment Effect (ATE).\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): Feature matrix for ATE estimation.\n",
    "\n",
    "        Returns:\n",
    "            float: Estimated ATE.\n",
    "        \"\"\"\n",
    "        cate_pred = self.predict(X, return_components=False)\n",
    "        # Average the predictions\n",
    "        return float(np.mean(cate_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "class LassoSplineLearner:\n",
    "    def __init__(self, df_spline=7, cv=5, degree=2):\n",
    "        \"\"\"\n",
    "        Lasso model with natural spline and polynomial interactions.\n",
    "        \n",
    "        df_spline: Degrees of freedom for the natural spline.\n",
    "        cv: Number of cross-validation folds for lasso.\n",
    "        degree: Degree of polynomial expansion.\n",
    "        \"\"\"\n",
    "        self.df_spline = df_spline\n",
    "        self.cv = cv\n",
    "        self.degree = degree\n",
    "        self.lasso = None \n",
    "\n",
    "    def get_params(self):\n",
    "        return self.df_spline, self.cv, self.degree\n",
    "    \n",
    "    def _transform_features(self, X):\n",
    "        # Apply natural spline expansion and polynomial interactions\n",
    "        df = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(X.shape[1])])\n",
    "        # Create spline transformations for each feature individually\n",
    "        spline_terms = []\n",
    "        for i in range(X.shape[1]):\n",
    "            spline_terms.append(f\"bs(X{i}, df={self.df_spline}, include_intercept=False)\")\n",
    "        \n",
    "        formula = \" + \".join(spline_terms)  # Combine all spline transformations\n",
    "        spline_basis = dmatrix(formula, data=df, return_type='dataframe')\n",
    "\n",
    "        # Apply polynomial interactions\n",
    "        poly = PolynomialFeatures(degree=self.degree, interaction_only=False, include_bias=True)\n",
    "        return poly.fit_transform(spline_basis)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit Lasso on transformed features\n",
    "        X_transformed = self._transform_features(X)\n",
    "        self.lasso = Lasso().fit(X_transformed, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict using trained lasso model\n",
    "        if self.lasso is None:\n",
    "            raise ValueError(\"Model is not trained. Call .fit() first.\")\n",
    "        X_transformed = self._transform_features(X)\n",
    "        return self.lasso.predict(X_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities and helpers for retrieving the datasets\n",
    "\"\"\"\n",
    "# stdlib\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import os\n",
    "\n",
    "\n",
    "def download_gdrive_if_needed(path: Path, file_id: str) -> None:\n",
    "    \"\"\"\n",
    "    Helper for downloading a file from Google Drive, if it is now already on the disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: Path\n",
    "        Where to download the file\n",
    "    file_id: str\n",
    "        Google Drive File ID. Details: https://developers.google.com/drive/api/v3/about-files\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "\n",
    "    gdown.download(id=file_id, output=str(path), quiet=False)\n",
    "\n",
    "\n",
    "def download_http_if_needed(path: Path, url: str) -> None:\n",
    "    \"\"\"\n",
    "    Helper for downloading a file, if it is now already on the disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: Path\n",
    "        Where to download the file.\n",
    "    url: URL string\n",
    "        HTTP URL for the dataset.\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "\n",
    "    if path.exists():\n",
    "        return\n",
    "\n",
    "    if url.lower().startswith(\"http\"):\n",
    "        urllib.request.urlretrieve(url, path)  # nosec\n",
    "        return\n",
    "\n",
    "    raise ValueError(f\"Invalid url provided {url}\")\n",
    "\n",
    "\n",
    "def unarchive_if_needed(path: Path, output_folder: Path) -> None:\n",
    "    \"\"\"\n",
    "    Helper for uncompressing archives. Supports .tar.gz and .tar.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: Path\n",
    "        Source archive.\n",
    "    output_folder: Path\n",
    "        Where to unarchive.\n",
    "    \"\"\"\n",
    "    if str(path).endswith(\".tar.gz\"):\n",
    "        tar = tarfile.open(path, \"r:gz\")\n",
    "        tar.extractall(path=output_folder) # nosec\n",
    "        tar.close()\n",
    "    elif str(path).endswith(\".tar\"):\n",
    "        tar = tarfile.open(path, \"r:\")\n",
    "        tar.extractall(path=output_folder) # nosec\n",
    "        tar.close()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"archive not supported {path}\")\n",
    "\n",
    "\n",
    "def download_if_needed(\n",
    "    download_path: Path,\n",
    "    file_id: Optional[str] = None,  # used for downloading from Google Drive\n",
    "    http_url: Optional[str] = None,  # used for downloading from a HTTP URL\n",
    "    unarchive: bool = False,  # unzip a downloaded archive\n",
    "    unarchive_folder: Optional[Path] = None,  # unzip folder\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Helper for retrieving online datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    download_path: str\n",
    "        Where to download the archive\n",
    "    file_id: str, optional\n",
    "        Set this if you want to download from a public Google drive share\n",
    "    http_url: str, optional\n",
    "        Set this if you want to download from a HTTP URL\n",
    "    unarchive: bool\n",
    "        Set this if you want to try to unarchive the downloaded file\n",
    "    unarchive_folder: str\n",
    "        Mandatory if you set unarchive to True.\n",
    "    \"\"\"\n",
    "    download_path = Path(download_path)\n",
    "    if file_id is not None:\n",
    "        download_gdrive_if_needed(download_path, file_id)\n",
    "    elif http_url is not None:\n",
    "        download_http_if_needed(download_path, http_url)\n",
    "    else:\n",
    "        raise ValueError(\"Please provide a download URL\")\n",
    "\n",
    "    if unarchive and unarchive_folder is None:\n",
    "        raise ValueError(\"Please provide a folder for the archive\")\n",
    "    if unarchive and unarchive_folder is not None:\n",
    "        try:\n",
    "            unarchive_if_needed(download_path, unarchive_folder)\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to unpack {download_path}. Error {e}\")\n",
    "            download_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IHDP (Infant Health and Development Program) dataset\n",
    "\"\"\"\n",
    "# stdlib\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, Tuple\n",
    "\n",
    "TRAIN_DATASET = \"ihdp_npci_1-100.train.npz\"\n",
    "TEST_DATASET = \"ihdp_npci_1-100.test.npz\"\n",
    "TRAIN_URL = \"https://www.fredjo.com/files/ihdp_npci_1-100.train.npz\"\n",
    "TEST_URL = \"https://www.fredjo.com/files/ihdp_npci_1-100.test.npz\"\n",
    "\n",
    "\n",
    "# helper functions\n",
    "def load_data_npz(fname: Path, get_po: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Helper function for loading the IHDP data set (adapted from https://github.com/clinicalml/cfrnet)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname: Path\n",
    "        Dataset path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: dict\n",
    "        Raw IHDP dict, with X, w, y and yf keys.\n",
    "    \"\"\"\n",
    "    data_in = np.load(fname)\n",
    "    data = {\"X\": data_in[\"x\"], \"w\": data_in[\"t\"], \"y\": data_in[\"yf\"]}\n",
    "    try:\n",
    "        data[\"ycf\"] = data_in[\"ycf\"]\n",
    "    except BaseException:\n",
    "        data[\"ycf\"] = None\n",
    "\n",
    "    if get_po:\n",
    "        data[\"mu0\"] = data_in[\"mu0\"]\n",
    "        data[\"mu1\"] = data_in[\"mu1\"]\n",
    "\n",
    "    data[\"HAVE_TRUTH\"] = not data[\"ycf\"] is None\n",
    "    data[\"dim\"] = data[\"X\"].shape[1]\n",
    "    data[\"n\"] = data[\"X\"].shape[0]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_ihdp_data(\n",
    "    data_train: dict,\n",
    "    data_test: dict,\n",
    "    rescale: bool = False,\n",
    "    setting: str = \"C\",\n",
    "    return_pos: bool = False,\n",
    ") -> Tuple:\n",
    "    \"\"\"\n",
    "    Helper for preprocessing the IHDP dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_train: pd.DataFrame or dict\n",
    "        Train dataset\n",
    "    data_test: pd.DataFrame or dict\n",
    "        Test dataset\n",
    "    rescale: bool, default False\n",
    "        Rescale the outcomes to have similar scale\n",
    "    setting: str, default C\n",
    "        Experiment setting\n",
    "    return_pos: bool\n",
    "        Return potential outcomes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X: dict or pd.DataFrame\n",
    "        Training Feature set\n",
    "    y: pd.DataFrame or list\n",
    "        Outcome list\n",
    "    t: pd.DataFrame or list\n",
    "        Treatment list\n",
    "    cate_true_in: pd.DataFrame or list\n",
    "        Average treatment effects for the training set\n",
    "    X_t: pd.Dataframe or list\n",
    "        Test feature set\n",
    "    cate_true_out: pd.DataFrame of list\n",
    "        Average treatment effects for the testing set\n",
    "    \"\"\"\n",
    "\n",
    "    X, y, w, mu0, mu1 = (\n",
    "        data_train[\"X\"],\n",
    "        data_train[\"y\"],\n",
    "        data_train[\"w\"],\n",
    "        data_train[\"mu0\"],\n",
    "        data_train[\"mu1\"],\n",
    "    )\n",
    "\n",
    "    X_t, y_t, w_t, mu0_t, mu1_t = (\n",
    "        data_test[\"X\"],\n",
    "        data_test[\"y\"],\n",
    "        data_test[\"w\"],\n",
    "        data_test[\"mu0\"],\n",
    "        data_test[\"mu1\"],\n",
    "    )\n",
    "    if setting == \"D\":\n",
    "        y[w == 1] = y[w == 1] + mu0[w == 1]\n",
    "        mu1 = mu0 + mu1\n",
    "        mu1_t = mu0_t + mu1_t\n",
    "\n",
    "    if rescale:\n",
    "        # rescale all outcomes to have similar scale of CATEs if sd_cate > 1\n",
    "        cate_in = mu0 - mu1\n",
    "        sd_cate = np.sqrt(cate_in.var())\n",
    "\n",
    "        if sd_cate > 1:\n",
    "            # training data\n",
    "            error = y - w * mu1 - (1 - w) * mu0\n",
    "            mu0 = mu0 / sd_cate\n",
    "            mu1 = mu1 / sd_cate\n",
    "            y = w * mu1 + (1 - w) * mu0 + error\n",
    "\n",
    "            # test data\n",
    "            mu0_t = mu0_t / sd_cate\n",
    "            mu1_t = mu1_t / sd_cate\n",
    "\n",
    "    cate_true_in = mu1 - mu0\n",
    "    cate_true_out = mu1_t - mu0_t\n",
    "\n",
    "    if return_pos:\n",
    "        return X, y, w, cate_true_in, X_t, y_t, w_t, cate_true_out, mu0, mu1, mu0_t, mu1_t\n",
    "\n",
    "    else:\n",
    "        return X, y, w, cate_true_in, X_t, y_t, w_t, cate_true_out\n",
    "\n",
    "\n",
    "def prepare_ihdp_pairnet_data(\n",
    "    i_exp,\n",
    "    model_name: str,\n",
    "    data_train: dict,\n",
    "    data_test: dict,\n",
    "    rescale: bool = False,\n",
    "    setting: str = \"C\",\n",
    "    return_pos: bool = False,\n",
    "    **kwargs,\n",
    "):\n",
    "    X, y, beta, cate_true_in, X_test, beta_test, cate_true_out = prepare_ihdp_data(\n",
    "        data_train,\n",
    "        data_test,\n",
    "        rescale=rescale,\n",
    "        setting=setting,\n",
    "        return_pos=return_pos,\n",
    "    )\n",
    "\n",
    "\n",
    "    ads_train = None\n",
    "\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"X\": X,\n",
    "            \"y\": y,\n",
    "            \"w\": beta,\n",
    "            \"cate_true_in\": cate_true_in,\n",
    "            \"X_t\": X_test,\n",
    "            \"w_t\": beta_test,\n",
    "            \"cate_true_out\": cate_true_out,\n",
    "        },\n",
    "        ads_train,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_one_data_set(D: dict, i_exp: int, get_po: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Helper for getting the IHDP data for one experiment. Adapted from https://github.com/clinicalml/cfrnet\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    D: dict or pd.DataFrame\n",
    "        All the experiment\n",
    "    i_exp: int\n",
    "        Experiment number\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: dict or pd.Dataframe\n",
    "        dict with the experiment\n",
    "    \"\"\"\n",
    "    D_exp = {}\n",
    "    D_exp[\"X\"] = D[\"X\"][:, :, i_exp - 1]\n",
    "    D_exp[\"w\"] = D[\"w\"][:, i_exp - 1 : i_exp]\n",
    "    D_exp[\"y\"] = D[\"y\"][:, i_exp - 1 : i_exp]\n",
    "    if D[\"HAVE_TRUTH\"]:\n",
    "        D_exp[\"ycf\"] = D[\"ycf\"][:, i_exp - 1 : i_exp]\n",
    "    else:\n",
    "        D_exp[\"ycf\"] = None\n",
    "\n",
    "    if get_po:\n",
    "        D_exp[\"mu0\"] = D[\"mu0\"][:, i_exp - 1 : i_exp]\n",
    "        D_exp[\"mu1\"] = D[\"mu1\"][:, i_exp - 1 : i_exp]\n",
    "\n",
    "    return D_exp\n",
    "\n",
    "\n",
    "def load(data_path: Path, exp: int = 1, rescale: bool = False, **kwargs: Any) -> Tuple:\n",
    "    \"\"\"\n",
    "    Get IHDP train/test datasets with treatments and labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path: Path\n",
    "        Path to the dataset csv. If the data is missing, it will be downloaded.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X: pd.Dataframe or array\n",
    "        The training feature set\n",
    "    w: pd.DataFrame or array\n",
    "        Training treatment assignments.\n",
    "    y: pd.Dataframe or array\n",
    "        The training labels\n",
    "    training potential outcomes: pd.DataFrame or array.\n",
    "        Potential outcomes for the training set.\n",
    "    X_t: pd.DataFrame or array\n",
    "        The testing feature set\n",
    "    testing potential outcomes: pd.DataFrame of array\n",
    "        Potential outcomes for the testing set.\n",
    "    \"\"\"\n",
    "    data_train, data_test = load_raw(data_path)\n",
    "\n",
    "    data_exp = get_one_data_set(data_train, i_exp=exp, get_po=True)\n",
    "    data_exp_test = get_one_data_set(data_test, i_exp=exp, get_po=True)\n",
    "\n",
    "    (\n",
    "        X,\n",
    "        y,\n",
    "        w,\n",
    "        cate_true_in,\n",
    "        X_t,\n",
    "        w_t,\n",
    "        cate_true_out,\n",
    "        mu0,\n",
    "        mu1,\n",
    "        mu0_t,\n",
    "        mu1_t,\n",
    "    ) = prepare_ihdp_data(\n",
    "        data_exp,\n",
    "        data_exp_test,\n",
    "        rescale=rescale,\n",
    "        return_pos=True,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X,\n",
    "        w,\n",
    "        y,\n",
    "        np.asarray([mu0, mu1]).squeeze().T,\n",
    "        X_t,\n",
    "        np.asarray([mu0_t, mu1_t]).squeeze().T,\n",
    "    )\n",
    "\n",
    "\n",
    "def load_raw(data_path: Path) -> Tuple:\n",
    "    \"\"\"\n",
    "    Get IHDP raw train/test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path: Path\n",
    "        Path to the dataset csv. If the data is missing, it will be downloaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    data_train: dict or pd.DataFrame\n",
    "        Training data\n",
    "    data_test: dict or pd.DataFrame\n",
    "        Testing data\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        os.mkdir(data_path)\n",
    "    except BaseException:\n",
    "        pass\n",
    "\n",
    "    train_csv = data_path / TRAIN_DATASET\n",
    "    test_csv = data_path / TEST_DATASET\n",
    "\n",
    "    download_if_needed(train_csv, http_url=TRAIN_URL)\n",
    "    download_if_needed(test_csv, http_url=TEST_URL)\n",
    "\n",
    "    data_train = load_data_npz(train_csv, get_po=True)\n",
    "    data_test = load_data_npz(test_csv, get_po=True)\n",
    "\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairnet_nn(input_dim, hidden_dims, epochs):\n",
    "    pairnet_nn = PairNetTorch(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=hidden_dims, \n",
    "        learning_rate=0.001,\n",
    "        weight_decay=0.0001,\n",
    "        epochs=epochs, \n",
    "        batch_size=200, \n",
    "        distance_threshold=np.inf,\n",
    "        num_neighbours=3,\n",
    "        val_size=0.1, \n",
    "        patience=10,\n",
    "        verbose=epochs+1,\n",
    "    )\n",
    "    return pairnet_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, iter=None):\n",
    "    if dataset == 'ihdp_100':\n",
    "        data_train, data_test = load_raw(Path(\"IHDP/\"))\n",
    "        X_train, y_train, treatment_train, tau_train, X_val, y_val, treatment_val, tau_val = prepare_ihdp_data(data_train, data_test)\n",
    "        X_train = X_train.reshape(-1, X_train.shape[1])\n",
    "        y_train = y_train.reshape(-1)\n",
    "        treatment_train = treatment_train.reshape(-1)\n",
    "        tau_train = tau_train.reshape(-1)\n",
    "        X_val = X_val.reshape(-1, X_val.shape[1])\n",
    "        treatment_val = treatment_val.reshape(-1)\n",
    "        y_val = y_val.reshape(-1)\n",
    "        tau_val = tau_val.reshape(-1)\n",
    "\n",
    "    elif dataset == 'ihdp_b':\n",
    "        # load all ihdp_b data\n",
    "        df_train = pd.DataFrame()\n",
    "        df_test = pd.DataFrame()\n",
    "        if iter == None:\n",
    "            for i in range(1, 10):\n",
    "                data = pd.read_csv('IHDP/csv_2/ihdp_npci_train_' + str(i) + '.csv', header=0)\n",
    "                df_train = pd.concat([data, df_train])\n",
    "                data = pd.read_csv('IHDP/csv_2/ihdp_npci_test_' + str(i) + '.csv', header=0)\n",
    "                df_test = pd.concat([data, df_test])\n",
    "        else:\n",
    "            data = pd.read_csv('IHDP/csv_2/ihdp_npci_train_' + str(iter) + '.csv', header=0)\n",
    "            df_train = pd.concat([data, df_train])\n",
    "            data = pd.read_csv('IHDP/csv_2/ihdp_npci_test_' + str(iter) + '.csv', header=0)\n",
    "            df_test = pd.concat([data, df_test])\n",
    "        df_train = df_train.reset_index(drop=True)\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "        perm = ['x' + str(i) for i in range(1,26)]\n",
    "        \n",
    "        X_train = df_train[perm]\n",
    "        X_val = df_test[perm]\n",
    "\n",
    "        treatment_train = df_train['treatment'].values\n",
    "        treatment_val = df_test['treatment'].values\n",
    "\n",
    "        y_train = df_train['y_factual'].values\n",
    "        y_val = df_test['y_factual'].values\n",
    "\n",
    "        tau_train = df_train.apply(lambda d: d['y_factual'] - d['y_cfactual'] if d['treatment']==1\n",
    "                    else d['y_cfactual'] - d['y_factual'],\n",
    "                    axis=1)\n",
    "        tau_val = df_test.apply(lambda d: d['y_factual'] - d['y_cfactual'] if d['treatment']==1\n",
    "                    else d['y_cfactual'] - d['y_factual'],\n",
    "                    axis=1)\n",
    "\n",
    "    elif dataset == 'ihdp':\n",
    "        # load all ihdp data\n",
    "        df = pd.DataFrame()\n",
    "        if iter == None:\n",
    "            for i in range(1, 10):\n",
    "                data = pd.read_csv('IHDP/csv/ihdp_npci_' + str(i) + '.csv', header=None)\n",
    "                df = pd.concat([data, df])\n",
    "        else:\n",
    "            data = pd.read_csv('IHDP/csv/ihdp_npci_' + str(iter) + '.csv', header=None)\n",
    "            df = pd.concat([data, df])\n",
    "        cols =  [\"treatment\", \"y_factual\", \"y_cfactual\", \"mu0\", \"mu1\"] + [i for i in range(25)]\n",
    "        df.columns = cols\n",
    "\n",
    "        # set which features are binary\n",
    "        binfeats = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
    "        # set which features are continuous\n",
    "        contfeats = [i for i in range(25) if i not in binfeats]\n",
    "        # reorder features with binary first and continuous after\n",
    "        perm = binfeats + contfeats\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        X = df[perm].values\n",
    "        treatment = df['treatment'].values\n",
    "        y = df['y_factual'].values\n",
    "        y_cf = df['y_cfactual'].values\n",
    "        tau = df.apply(lambda d: d['y_factual'] - d['y_cfactual'] if d['treatment']==1\n",
    "                    else d['y_cfactual'] - d['y_factual'],\n",
    "                    axis=1)\n",
    "        mu_0 = df['mu0'].values\n",
    "        mu_1 = df['mu1'].values\n",
    "        \n",
    "        # seperate for train and test\n",
    "        itr, ite = train_test_split(np.arange(X.shape[0]), test_size=0.2, random_state=1)\n",
    "        X_train, treatment_train, y_train, y_cf_train, tau_train, mu_0_train, mu_1_train = X[itr], treatment[itr], y[itr], y_cf[itr], tau[itr], mu_0[itr], mu_1[itr]\n",
    "        X_val, treatment_val, y_val, y_cf_val, tau_val, mu_0_val, mu_1_val = X[ite], treatment[ite], y[ite], y_cf[ite], tau[ite], mu_0[ite], mu_1[ite]\n",
    "\n",
    "    elif dataset == 'twins':\n",
    "        df = pd.read_csv(\"twins/twins.csv\", index_col=0)\n",
    "        treatment = df['T'].to_numpy().reshape(-1)\n",
    "        y = df.yf.to_numpy().reshape(-1)\n",
    "        y_cf = df.y_cf.to_numpy().reshape(-1)\n",
    "        mu_0 = df.y0.to_numpy().reshape(-1)\n",
    "        mu_1 = df.y1.to_numpy().reshape(-1)\n",
    "        tau = mu_1 - mu_0\n",
    "        X = df.drop(columns=['T', 'y0', 'y1', 'yf', 'y_cf', 'Propensity']).to_numpy()\n",
    "        itr, ite = train_test_split(np.arange(X.shape[0]), test_size=0.5)\n",
    "        X_train, treatment_train, y_train, y_cf_train, tau_train, mu_0_train, mu_1_train = X[itr], treatment[itr], y[itr], y_cf[itr], tau[itr], mu_0[itr], mu_1[itr]\n",
    "        X_val, treatment_val, y_val, y_cf_val, tau_val, mu_0_val, mu_1_val = X[ite], treatment[ite], y[ite], y_cf[ite], tau[ite], mu_0[ite], mu_1[ite]\n",
    "\n",
    "    return X_train, y_train, treatment_train, tau_train, X_val, y_val, treatment_val, tau_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propensity(X_train, treatment_train, X_val, treatment_val):\n",
    "    # fit propensity model\n",
    "    p_model = LogisticRegression(max_iter=1000)\n",
    "    p_train = p_model.fit(X_train, treatment_train).predict_proba(X_train)[:,1]\n",
    "    p_val = p_model.predict_proba(X_val)[:,1]\n",
    "    return p_train, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(learner_dict, num_iter, epochs, dataset):\n",
    "    result_list = []\n",
    "\n",
    "    pairnet_time = np.inf\n",
    "\n",
    "    learners = ['S-learner', 'T-learner', 'X-learner', 'DR-learner', 'RT-learner', 'P-learner', 'RP-learner', 'PairNet']\n",
    "\n",
    "    for i in tqdm(range(num_iter)):\n",
    "        \n",
    "        # Get data, either IHDP or Twins\n",
    "        X_train, y_train, treatment_train, tau_train, X_val, y_val, treatment_val, tau_val = get_data(dataset, iter=i%50 + 1)\n",
    "        # Fit propensity score models\n",
    "        p_train, p_val = get_propensity(X_train, treatment_train, X_val, treatment_val)\n",
    "\n",
    "        for learner in learner_dict.keys():\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            # Fit models\n",
    "            if learner != 'PairNet':\n",
    "                model = deepcopy(learner_dict[learner])\n",
    "                model.fit(X_train, treatment_train, y_train, p_train)\n",
    "\n",
    "            else:\n",
    "                model = create_pairnet_nn(X_train.shape[1], (100,100), epochs)\n",
    "                model.fit(X_train, treatment_train, y_train, total_time=pairnet_time)\n",
    "            \n",
    "            # Calculate PEHE on training and test sets\n",
    "            try:\n",
    "                pehe_in = mean_squared_error(model.predict(X_train, p=p_train), tau_train)\n",
    "                pehe_out = mean_squared_error(model.predict(X_val, p=p_val), tau_val)\n",
    "            except:\n",
    "                pehe_in = mean_squared_error(model.predict(X_train), tau_train)\n",
    "                pehe_out = mean_squared_error(model.predict(X_val), tau_val)\n",
    "\n",
    "            elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "            # If P-learner run, limit time to train PairNet\n",
    "            if learner == 'P-learner':\n",
    "                pairnet_time = elapsed_time\n",
    "\n",
    "            result_list.append([learner, pehe_in, pehe_out, elapsed_time])  \n",
    "\n",
    "\n",
    "    cols = ['learner', 'pehe_in', 'pehe_out', 'time']\n",
    "    df_res = pd.DataFrame(result_list, columns=cols)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:14<00:00, 49.49s/it]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "dataset = 'ihdp_b'\n",
    "\n",
    "learners = [RandomForestRegressor(max_depth=10), \n",
    "            MLPRegressor((100,100), max_iter=epochs, early_stopping=True, validation_fraction=0.1)]\n",
    "\n",
    "for learner in learners:\n",
    "    dfs = []\n",
    "\n",
    "    num_iter = 100\n",
    "\n",
    "    try:\n",
    "        if learner.n_layers_ < np.inf:\n",
    "            learner_dict = {\n",
    "                'S-learner': BaseSRegressor(learner),\n",
    "                'T-learner': BaseTRegressor(learner),\n",
    "                'X-learner': BaseXRegressor(learner),\n",
    "                #'R-learner': BaseRRegressor(learner),\n",
    "                'DR-learner': BaseDRRegressor(learner),\n",
    "                'RT-learner': BaseRTRegressor(learner),\n",
    "                'P-learner': BasePRegressor(learner),\n",
    "                'PairNet': ''\n",
    "            }\n",
    "    except:\n",
    "        learner_dict = {\n",
    "            'S-learner': BaseSRegressor(learner),\n",
    "            'T-learner': BaseTRegressor(learner),\n",
    "            'X-learner': BaseXRegressor(learner),\n",
    "            #'R-learner': BaseRRegressor(learner),\n",
    "            'DR-learner': BaseDRRegressor(learner),\n",
    "            'RT-learner': BaseRTRegressor(learner),\n",
    "            'P-learner': BasePRegressor(learner),\n",
    "            #'PairNet': ''\n",
    "        }\n",
    "\n",
    "    dfs.append(run_experiments(learner_dict, num_iter, epochs, dataset))\n",
    "\n",
    "df_rf, df_nn = dfs[0], df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res_nn_ihdp_b['root_pehe_in'] = np.sqrt(df_res_nn_ihdp_b['pehe_in'])\n",
    "df_res_nn_ihdp_b['root_pehe_out'] = np.sqrt(df_res_nn_ihdp_b['pehe_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5d4db_row0_col9, #T_5d4db_row1_col9, #T_5d4db_row2_col11 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5d4db\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5d4db_level0_col0\" class=\"col_heading level0 col0\" >index</th>\n",
       "      <th id=\"T_5d4db_level0_col1\" class=\"col_heading level0 col1\" colspan=\"7\">mean</th>\n",
       "      <th id=\"T_5d4db_level0_col8\" class=\"col_heading level0 col8\" colspan=\"7\">sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >learner</th>\n",
       "      <th id=\"T_5d4db_level1_col0\" class=\"col_heading level1 col0\" ></th>\n",
       "      <th id=\"T_5d4db_level1_col1\" class=\"col_heading level1 col1\" >DR-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col2\" class=\"col_heading level1 col2\" >P-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col3\" class=\"col_heading level1 col3\" >PairNet</th>\n",
       "      <th id=\"T_5d4db_level1_col4\" class=\"col_heading level1 col4\" >RT-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col5\" class=\"col_heading level1 col5\" >S-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col6\" class=\"col_heading level1 col6\" >T-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col7\" class=\"col_heading level1 col7\" >X-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col8\" class=\"col_heading level1 col8\" >DR-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col9\" class=\"col_heading level1 col9\" >P-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col10\" class=\"col_heading level1 col10\" >PairNet</th>\n",
       "      <th id=\"T_5d4db_level1_col11\" class=\"col_heading level1 col11\" >RT-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col12\" class=\"col_heading level1 col12\" >S-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col13\" class=\"col_heading level1 col13\" >T-learner</th>\n",
       "      <th id=\"T_5d4db_level1_col14\" class=\"col_heading level1 col14\" >X-learner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5d4db_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5d4db_row0_col0\" class=\"data row0 col0\" >pehe_in</td>\n",
       "      <td id=\"T_5d4db_row0_col1\" class=\"data row0 col1\" >14.601</td>\n",
       "      <td id=\"T_5d4db_row0_col2\" class=\"data row0 col2\" >5.050</td>\n",
       "      <td id=\"T_5d4db_row0_col3\" class=\"data row0 col3\" >6.239</td>\n",
       "      <td id=\"T_5d4db_row0_col4\" class=\"data row0 col4\" >10.164</td>\n",
       "      <td id=\"T_5d4db_row0_col5\" class=\"data row0 col5\" >12.813</td>\n",
       "      <td id=\"T_5d4db_row0_col6\" class=\"data row0 col6\" >12.591</td>\n",
       "      <td id=\"T_5d4db_row0_col7\" class=\"data row0 col7\" >11.532</td>\n",
       "      <td id=\"T_5d4db_row0_col8\" class=\"data row0 col8\" >0.469</td>\n",
       "      <td id=\"T_5d4db_row0_col9\" class=\"data row0 col9\" >0.083</td>\n",
       "      <td id=\"T_5d4db_row0_col10\" class=\"data row0 col10\" >0.117</td>\n",
       "      <td id=\"T_5d4db_row0_col11\" class=\"data row0 col11\" >0.544</td>\n",
       "      <td id=\"T_5d4db_row0_col12\" class=\"data row0 col12\" >0.576</td>\n",
       "      <td id=\"T_5d4db_row0_col13\" class=\"data row0 col13\" >0.658</td>\n",
       "      <td id=\"T_5d4db_row0_col14\" class=\"data row0 col14\" >0.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5d4db_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5d4db_row1_col0\" class=\"data row1 col0\" >pehe_out</td>\n",
       "      <td id=\"T_5d4db_row1_col1\" class=\"data row1 col1\" >15.228</td>\n",
       "      <td id=\"T_5d4db_row1_col2\" class=\"data row1 col2\" >6.900</td>\n",
       "      <td id=\"T_5d4db_row1_col3\" class=\"data row1 col3\" >8.411</td>\n",
       "      <td id=\"T_5d4db_row1_col4\" class=\"data row1 col4\" >11.089</td>\n",
       "      <td id=\"T_5d4db_row1_col5\" class=\"data row1 col5\" >13.770</td>\n",
       "      <td id=\"T_5d4db_row1_col6\" class=\"data row1 col6\" >13.860</td>\n",
       "      <td id=\"T_5d4db_row1_col7\" class=\"data row1 col7\" >12.285</td>\n",
       "      <td id=\"T_5d4db_row1_col8\" class=\"data row1 col8\" >0.509</td>\n",
       "      <td id=\"T_5d4db_row1_col9\" class=\"data row1 col9\" >0.110</td>\n",
       "      <td id=\"T_5d4db_row1_col10\" class=\"data row1 col10\" >0.153</td>\n",
       "      <td id=\"T_5d4db_row1_col11\" class=\"data row1 col11\" >0.547</td>\n",
       "      <td id=\"T_5d4db_row1_col12\" class=\"data row1 col12\" >0.563</td>\n",
       "      <td id=\"T_5d4db_row1_col13\" class=\"data row1 col13\" >0.678</td>\n",
       "      <td id=\"T_5d4db_row1_col14\" class=\"data row1 col14\" >0.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5d4db_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5d4db_row2_col0\" class=\"data row2 col0\" >time</td>\n",
       "      <td id=\"T_5d4db_row2_col1\" class=\"data row2 col1\" >1.172</td>\n",
       "      <td id=\"T_5d4db_row2_col2\" class=\"data row2 col2\" >1.651</td>\n",
       "      <td id=\"T_5d4db_row2_col3\" class=\"data row2 col3\" >1.682</td>\n",
       "      <td id=\"T_5d4db_row2_col4\" class=\"data row2 col4\" >0.871</td>\n",
       "      <td id=\"T_5d4db_row2_col5\" class=\"data row2 col5\" >0.801</td>\n",
       "      <td id=\"T_5d4db_row2_col6\" class=\"data row2 col6\" >0.839</td>\n",
       "      <td id=\"T_5d4db_row2_col7\" class=\"data row2 col7\" >1.589</td>\n",
       "      <td id=\"T_5d4db_row2_col8\" class=\"data row2 col8\" >0.040</td>\n",
       "      <td id=\"T_5d4db_row2_col9\" class=\"data row2 col9\" >0.051</td>\n",
       "      <td id=\"T_5d4db_row2_col10\" class=\"data row2 col10\" >0.052</td>\n",
       "      <td id=\"T_5d4db_row2_col11\" class=\"data row2 col11\" >0.023</td>\n",
       "      <td id=\"T_5d4db_row2_col12\" class=\"data row2 col12\" >0.027</td>\n",
       "      <td id=\"T_5d4db_row2_col13\" class=\"data row2 col13\" >0.029</td>\n",
       "      <td id=\"T_5d4db_row2_col14\" class=\"data row2 col14\" >0.048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x322ad3250>"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivot table to restructure the dataframe\n",
    "df_pivot = df_res_nn_ihdp_b.pivot_table(\n",
    "    index=[],  # Grouping columns\n",
    "    columns=\"learner\",          # Learner categories become new columns\n",
    "    values=[\"pehe_in\", 'pehe_out', 'time'],              \n",
    "    aggfunc=[\"mean\", 'sem']            \n",
    ").reset_index()\n",
    "\n",
    "# Rename columns if needed\n",
    "df_pivot.columns.name = None  # Remove the automatic column name\n",
    "\n",
    "# Function to apply bold formatting\n",
    "def highlight_min(s):\n",
    "    is_min = s == s.min()\n",
    "    return [\"font-weight: bold\" if v else \"\" for v in is_min]\n",
    "\n",
    "# Apply formatting to only the learner columns\n",
    "learner_columns = df_pivot.columns[1:]\n",
    "styled_df = df_pivot.style.format(precision=3).apply(highlight_min, subset=learner_columns, axis=1)\n",
    "\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{tabular}{llrrrrrrrrrrrrrr}\\n & index & \\\\multicolumn{7}{r}{mean} & \\\\multicolumn{7}{r}{sem} \\\\\\\\\\nlearner &  & DR-learner & P-learner & PairNet & RT-learner & S-learner & T-learner & X-learner & DR-learner & P-learner & PairNet & RT-learner & S-learner & T-learner & X-learner \\\\\\\\\\n0 & pehe_in & 14.601 & 5.050 & 6.239 & 10.164 & 12.813 & 12.591 & 11.532 & 0.469 & \\\\font-weightbold 0.083 & 0.117 & 0.544 & 0.576 & 0.658 & 0.648 \\\\\\\\\\n1 & pehe_out & 15.228 & 6.900 & 8.411 & 11.089 & 13.770 & 13.860 & 12.285 & 0.509 & \\\\font-weightbold 0.110 & 0.153 & 0.547 & 0.563 & 0.678 & 0.665 \\\\\\\\\\n2 & time & 1.172 & 1.651 & 1.682 & 0.871 & 0.801 & 0.839 & 1.589 & 0.040 & 0.051 & 0.052 & \\\\font-weightbold 0.023 & 0.027 & 0.029 & 0.048 \\\\\\\\\\n\\\\end{tabular}\\n'"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styled_df.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = deepcopy(df_res_nn_ihdp_b_3)\n",
    "\n",
    "sns.reset_orig()\n",
    "\n",
    "# Count how many learners\n",
    "n_learners = df['learner'].nunique()\n",
    "n_rows = len(df)\n",
    "\n",
    "# Create rep numbers\n",
    "df['rep'] = [(i % (100 * n_learners)) // n_learners + 1 for i in range(n_rows)]\n",
    "\n",
    "# Plot\n",
    "#plt.style.use('seaborn-v0_8-deep')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df[df['rep'] <= 10], x='rep', y='pehe_out', hue='learner', dodge=True, palette='Paired',\n",
    "            hue_order=['S-learner','T-learner','RT-learner','X-learner','DR-learner','P-learner','PairNet'])\n",
    "plt.title('PEHE across IHDP Simulations for Different Meta-Learners (NN)')\n",
    "plt.xlabel('Outcome simulation')\n",
    "plt.ylabel('PEHE')\n",
    "#plt.yscale('log')\n",
    "plt.grid(True, linestyle='--', which='major', color='grey', alpha=.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learner\n",
       "P-learner    8\n",
       "T-learner    1\n",
       "X-learner    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How often each learner is best\n",
    "df.sort_values('pehe_out', ascending=True).groupby('rep').first().learner.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learner\n",
       "S-learner    8\n",
       "T-learner    1\n",
       "X-learner    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How often each learner is worst\n",
    "df.sort_values('pehe_out', ascending=False).groupby('rep').first().learner.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
